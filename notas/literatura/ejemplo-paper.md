---
paper: "Attention Is All You Need"
authors: "Vaswani et al."
year: 2017
tags: [literatura, deep-learning, transformers, ejemplo]
status: leido
---

# Attention Is All You Need

## üìã Metadata
- **Autores:** Vaswani, Shazeer, Parmar, et al.
- **A√±o:** 2017
- **Journal/Conference:** NIPS 2017
- **DOI/URL:** https://arxiv.org/abs/1706.03762
- **Zotero:** [[Referencias]]

## üéØ Problema que Resuelve
Los modelos de secuencia a secuencia anteriores depend√≠an de RNNs/LSTMs que eran dif√≠ciles de paralelizar y ten√≠an problemas con dependencias de largo alcance.

## üí° Contribuci√≥n Principal
Introduce la arquitectura **Transformer**, que usa exclusivamente mecanismos de atenci√≥n (self-attention) sin recurrencia, permitiendo mayor paralelizaci√≥n y mejor captura de dependencias.

## üî¨ Metodolog√≠a
- Multi-head self-attention
- Positional encoding
- Feed-forward networks
- Layer normalization
- Residual connections

## üìä Resultados Clave
- SOTA en traducci√≥n autom√°tica (WMT 2014 EN-DE: 28.4 BLEU)
- Entrenamiento mucho m√°s r√°pido que modelos recurrentes
- Mejor interpretabilidad a trav√©s de attention weights

## üí≠ Fortalezas
- Altamente paralelizable
- Captura dependencias de largo alcance
- Base para GPT, BERT, y modelos modernos

## ‚ö†Ô∏è Limitaciones
- Requiere m√°s memoria para secuencias largas (O(n¬≤))
- Necesita positional encoding expl√≠cito

## üîó Conexiones
Papers relacionados:
- [[BERT]] - Usa transformers bidireccionales
- [[GPT]] - Transformers autoregresivos

Conceptos clave:
- [[Self-Attention]]
- [[Multi-Head Attention]]

## üí° Ideas para mi Investigaci√≥n
- ¬øPuedo aplicar transformers a mi dominio espec√≠fico?
- Investigar variantes eficientes para secuencias largas

## üìù Citas Importantes
> "The Transformer is the first transduction model relying entirely on self-attention to compute representations of its input and output without using sequence-aligned RNNs or convolution."

---
**Estado:** #leido
**Relevancia:** ‚≠ê‚≠ê‚≠ê‚≠ê‚≠ê
