\pdfobjcompresslevel=0
\pdfcompresslevel=9
\pdfminorversion=7
\documentclass[letterpaper, 12, one column]{article}
%% packages %%

% === Rasterizar PDFs pesados automáticamente ===
\usepackage{graphicx,xifthen}
\newif\iffigdraft
% \figdrafttrue   % ← descomenta para compilar rápido (raster a 150 dpi)
% \figdraftfalse  % ← descomenta para calidad final (raster a 300 dpi)




\usepackage{tikz}
\usepackage{amsmath}

\usetikzlibrary{angles, quotes}

\usepackage{hyperref}
\usepackage{graphics,color}
\usepackage[table]{xcolor} 
\usepackage{tabularray}
\usepackage{epsfig} 
\usepackage{psfrag,epsfig}
\usepackage{graphics}                       % Soporte gráfico
\usepackage{changepage}
\usepackage{graphicx}   
\usetikzlibrary{arrows.meta}% Soport\e gráfico adicional
\usepackage{epstopdf}
\usepackage{float}
\usepackage{mathptmx} 
\usepackage{times} 
\usepackage{amsmath}
\usepackage{amssymb}  
\usepackage{booktabs}
\usepackage{bm}  
\usepackage{rotating} % <-- Asegúrate de incluir esto en el preámbulo
\usetikzlibrary{angles, quotes,shapes}
\usepackage{color}
\usepackage{multirow} 
\usepackage{cleveref}
\usepackage{cite}
\usepackage{arydshln}
\usepackage{tabularx}
\usepackage{booktabs}
\usepackage{microtype}
\usepackage{longtable}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage{subcaption}
%\usepackage[caption=false]{subfig}
\usepackage{tikz}
    \usetikzlibrary{positioning,mindmap,arrows.meta,bending}
    \usetikzlibrary{mindmap, backgrounds, calc} % Mindmap drawing library 
    \usetikzlibrary{positioning,shapes,shadows,arrows, shapes.geometric}
\usepackage{adjustbox}
\newcommand{\gc}[1]{\textcolor[rgb]{1.00,0.00,0.00}{#1}}
\providecommand{\var}[1]{{\ensuremath{var}}\{#1\}}
\DeclareMathOperator{\tire}{\negthinspace-\negthinspace}
\DeclareRobustCommand{\legend}[1]{%
	\textcolor{#1}{\rule{1ex}{2ex}}%
}
\newcommand{\lineplot}[1]{
	\textcolor{#1}{\rule{1.5ex}{0.3ex}}%
}

%\setbeamertemplate{itemize/enumerate body begin}{\large}
%\usepackage{enumitem}
\newcommand{\tikzmark}[1]{\tikz[baseline,remember picture] \coordinate (#1) {};}

\usepackage{booktabs,colortbl,pifont}
\definecolor{completed}{RGB}{220,235,220}   % light green
\definecolor{writing}{RGB}{255,235,200}     % light orange
\newcommand{\cmark}{\ding{52}}   % ✔
\newcommand{\wmark}{\ding{118}}  % ⌘ (for “writing” phase) – choose any
\renewcommand{\arraystretch}{1.15}
%% commands %%
\DeclareMathAlphabet{\mathantt}{OT1}{antt}{li}{it}
\DeclareMathAlphabet{\mathpzc}{OT1}{pzc}{m}{it}
\providecommand{\abs}[1]{\lvert#1\rvert}
\providecommand{\norm}[1]{\lVert#1\rVert} 
\providecommand{\ppunto}[2]{\langle#1,#2\rangle}
\providecommand{\dist}[2]{{ d}(#1, #2)} 
\providecommand{\promed}[1]{{\fam=8 E}\{#1\}}
\providecommand{\cov}[2]{{\fam=7 cov}\{#1, #2\}}
\providecommand{\gaus}[2]{{\fam=6 N}(#1 #2)}
\providecommand{\unif}[1]{{\fam=6 U}(#1 )} 
\providecommand{\ve}[1]{{\bm {#1}}} 
\providecommand{\mat}[1]{{\bm {#1}}} 
\providecommand{\est}[1]{{\widetilde {#1}}}
\providecommand{\diag}[1]{{\rm{diag}}(#1)}
\newcommand{\jc}[1]{\textcolor[rgb]{0.00,0.0,1.00}{#1}}


\usepackage{longtable}
\usepackage{lscape}
\usepackage{multirow}
\usepackage{array}
\usepackage{xcolor}
\usepackage{xcolor}
\usepackage{soul}
\usetikzlibrary{positioning, arrows.meta}
%\usepackage{subcaption}

%\usepackage[hyperindex, colorlinks, citecolor=blue, linkcolor=blue, breaklinks=true, bookmarksopen]{hyperref}
\newcommand{\fou}{\ensuremath{\mathcal{F}}}%Para crear la F de Fourier
\newcommand{\ent}{\ensuremath{\mathbb{Z}}}%Pa crear algo parecido a la Z de enteros
\newcommand{\Z}{\ensuremath{\mathbb{Z}}}%Pa crear algo parecido a la R de reales
\newenvironment{definición}{\it \vspace{.4cm}\hrule \vspace{.2cm}}{\par\sf \vspace{.2cm}\rm\hrule\vspace{.4cm}}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%% Custom Commands %%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\usepackage{tikz}
\usetikzlibrary{external}
\providecommand{\abs}[1]{\left| #1 \right|}
\providecommand{\norm}[1]{{\lVert #1 \rVert}_2}
\providecommand{\normH}[1]{{\lVert #1 \rVert}_\mathscr{H}}
\providecommand{\promedd}[2]{\mathbb{E}_{#1}\!\left\{#2\right\}}% op{\tiny }erador de promedio
\providecommand{\prodpunto}[2]{\left\langle #1,#2\right\rangle}

\providecommand{\paren}[1]{\!\left(\!#1\!\right)\!}
\newcommand{\T}{\top\!}
\newcommand{\p}{\prime\!}

\providecommand{\median}[1]{\text{median}\!\left(\!#1\!\right)}
\providecommand{\sign}[1]{\text{sign}\!\left(\!#1\!\right)}
\providecommand{\ve}[1]{{\bm{#1}}} %
\providecommand{\tr}[1]{{\operatorname{tr}\left({#1}\right)}}
\providecommand{\mat}[1]{{\bm{#1}}} %
\providecommand{\var}[1]{{\operatorname{var}}\left\{#1\right\}}
\providecommand{\map}[1]{\varphi\!\left(#1\right)}
\newcommand{\Real}{\mathbb{R}}
\newcommand{\N}{\mathbb{N}}
%\newcommand{\Z}{\mathbb{Z}}
\newcommand{\unos}{\ve{1}}
\providecommand{\kernel}[2]{\kappa\negthinspace\left(#1,\negthinspace#2\right)}
\DeclareMathOperator{\subconj}{\negthinspace\subset\negthinspace }
\DeclareMathOperator{\en}{\!\in\!}
\DeclareMathOperator{\igual}{\negthinspace=\negthinspace} 
\DeclareMathOperator{\diferente}{\negthinspace \neq \negthinspace}
% \DeclareMathOperator{\dist}{\operatorname{d}}

%\DeclareMathOperator{\xx}{\negthickspace\times\negthickspace}
\providecommand{\s}[1]{\negthinspace#1\negthinspace}%

\usepackage[normalem]{ulem}
%% Color text %%
\providecommand{\falta}[1]{\textcolor{green}{\underline{#1}}}
\providecommand{\duda}[1]{\textcolor{yellow}{{#1}}}
\providecommand{\am}[1]{\textcolor{green}{#1}}
\providecommand{\je}[1]{\textcolor{blue}{#1}}

% Note: pifont, \cmark and \xmark already defined at line 73-77
\definecolor{mypink1}{rgb}{0.858, 0.188, 0.478}
\usepackage{graphicx,xifthen,xstring} % <— necesario

\newif\iffigdraft
% \figdrafttrue   % ← modo rápido (150 dpi)
\figdraftfalse    % ← modo final (300 dpi)

\newcommand{\rasterpdffile}[3][]{%
  % Uso: \rasterpdffile[<dpi>]{ruta/figura.pdf}{<ancho>}
  \def\dpi{#1}%
  \ifthenelse{\isempty{#1}}{\iffigdraft\def\dpi{150}\else\def\dpi{300}\fi}{}%
  \StrBefore{#2}{.pdf}[\base]%
  \IfFileExists{\base.ras@\dpi.png}{}{%
    \immediate\write18{gs -q -dSAFER -dBATCH -dNOPAUSE -sDEVICE=pngalpha -r\dpi
      -dFirstPage=1 -dLastPage=1 -o "\base.ras@\dpi.png" "#2"}%
  }%
  \includegraphics[width=#3]{\base.ras@\dpi.png}%
}

 % {A multimodal deep learning approach with preserved interpretability to support remote sensing tasks}}

\title{Explainable Deep Learning Framework for Estimating and Generating Magnetic Domains from Hamiltonian Parameterss}
\author{\emph{PhD Thesis Proposal}\\ Juan Sebastián Méndez Rondón \\ \texttt{\small jumendezro@unal.edu.co} \\ \\ \\ 
Advisor: Prof. Andr\'es Marino \'Alvarez-Meza \\ \small{Universidad Nacional de Colombia - Sede Manizales} \\ \small{Facultad de Ingenier\'ia y Arquitectura} \\ \texttt{\scriptsize   amalvarezme@unal.edu.co} 
\\ \\ 
Co-Advisor: Prof. Jorge Ivan Montes\\ \small{Universidad Nacional de Colombia - Sede la paz}  \\ \texttt{\scriptsize  jimontesm@unal.edu.co } }

%Lack of interpretability
\date{Doctorado en Ingenier\'ia - Autom\'atica \\ Universidad Nacional de Colombia - Sede Manizales}
% ====== PREÁMBULO COMPATIBLE CON TUS TABLAS ======
\usepackage{booktabs,tabularx,array,changepage,float}
\usepackage{soul,xcolor}
\sethlcolor{yellow!20}

% Macros de ancho usadas por tus tablas
\newlength{\fulllength}  \setlength{\fulllength}{\textwidth}
\newlength{\extralength} \setlength{\extralength}{0pt}

% Columna centrada expandible "C" para tabularx
\newcolumntype{C}{>{\centering\arraybackslash}X}


\begin{document}
\maketitle
\section{Abstract}
The study of magnetic domain structures in nanoscale materials is essential for advancing
emerging technologies in spintronics, biomedicine, and energy storage \cite{Wang2022, Rarokar2024, Lak2021}. 
Magnetic nanoparticles and nanodots exhibit complex spin textures—such as vortices, skyrmions,
and stripe domains—that determine their functional behavior in applications ranging from magnetic
memory bits to targeted hyperthermia \cite{Wang2022, Lak2021, Zelent2021}. Understanding and controlling
these states requires a deep connection between the parameters of the magnetic Hamiltonian
(exchange, anisotropy, Dzyaloshinskii–Moriya interaction, external field, and temperature) and the
resulting spatial spin configurations \cite{Zelent2021, Ga2022, Fert2023}.

However, several fundamental challenges persist at the nanoscale. The first is the nonlinearity and
high dimensionality of the mapping between physical parameters and domain patterns, which prevents
direct analytical inversion \cite{WangMicromag2024, DeepMLHam2024}. The second is degeneracy:
distinct combinations of Hamiltonian parameters can yield visually similar configurations, complicating
inverse identification \cite{Feng2024, Liu2023}. Third, experimental limitations—such as noise, finite
probe resolution, tip–sample convolution in MFM, and sample variability—hinder quantitative
interpretation of MFM or SP-STM images \cite{Makarova2022, Bagchi2024}. Finally, the computational cost
of atomistic or micromagnetic simulations restricts large-scale parameter sweeps, limiting predictive
exploration of material regimes \cite{WangMicromag2024, MuMax3docs}.

Deep learning has emerged as a powerful tool to bridge this gap. The inverse problem—inferring
physical parameters from magnetic domain images—has been successfully addressed using CNNs and
explainable regression frameworks \cite{Kong2023, AIP2022, DeepMLHam2024}. The direct problem—
generating domain configurations from parameters—has been explored in materials science using
generative models such as VAEs and conditional GANs \cite{MicroVAE2023, Ronne2024}. These methods
can reproduce general morphologies but suffer from poor interpretability, instability under noise, limited
generalization, and lack of physical constraints \cite{Feng2024, Park2024, MicroVAE2023}.


% AGREGAR EL PARRAFO INTRODUCTORIO ASOCIADO A LAS TAREAS QUE SE VAN A GENERAR. -- > REVISAR CON ANDRES MARINO.

%% FIGURA DEL PROBLEMA INVERSO Y DIRECTO --CHECK

%% CAMBIAR EL PROBLEMA MIXTO DEGENERAY OF CONFIGURATIONS --CHECK
%% 2 parrafos uno que conceptualmente explique que es  y las posibles causas segun la fisica. --CHECK
%% LOW QUALITY DATA. 

%% explicabilidad fisica.
%% RESUMIR EN UNA TABLA O DIBUJO EL ESTADO DEL ARTE.
%% tabla con items para los proyectos futuros.




% TUTORIAL DE REDES INFORMADAS POR LA FISICA -- MIRAR COMO LLEVARLO A UN METODOLOGIA DE OPTIMAZCIÓN BAYESIANA. - QUE SEA EXPLICABLE. COMO PUEDO SINTONIZAR DICHOS CON INTERPRETABILIDAD.
%% CON UN EJERCICIO BASICO.


Recently, diffusion-based models have achieved state-of-the-art performance in conditional image
synthesis and inverse design across materials science \cite{Alverson2024, Chen2024, SyMat2023}. Yet,
their application to magnetic systems remains underexplored, and existing generative strategies rarely
enforce physical consistency—such as energy minimization or Hamiltonian symmetry—limiting their use
for predictive materials discovery \cite{Ronne2024, Park2024}.


This work proposes a unified deep learning framework for the direct and inverse estimation of magnetic Hamiltonian parameters and nanoscale domain configurations. Instead of relying on a fixed generative architecture, the framework will explore families of regularized and variational deep learning methodologies capable of integrating physical priors—such as Hamiltonian constraints, energy-based consistency, or symmetry-aware embeddings—to enhance robustness under degeneracy and low-quality data conditions. \cite{Sparks2024, Optuna2022}.


\section{Motivation}

%1.1. Contexto general: el magnetismo a nanoescala

The study of magnetic systems at the nanoscale has gained strategic importance in
modern materials science \cite{Wang2022, Lak2021}. In particular, magnetic nanoparticles (MNPs) and nanodots
constitute platforms where spin interactions and collective quantum properties give
rise to emergent phenomena of great scientific and technological interest \cite{Jiang2023, Rarokar2024}. At these
scales, the balance between exchange interactions, anisotropy, Dzyaloshinskii–Moriya
interaction (DMI), and external magnetic fields generates a wide variety of magnetic
textures—vortices, skyrmions, helical domains, and labyrinthine configurations \cite{Ga2022, Fert2023, Tokura2021}. These
structures determine the magnetic response, thermal stability, and switching dynamics
of the material, which are key aspects for functionality in advanced devices \cite{Song2023, Wang2022}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/magneticTexture.jpg}
    \caption{ Magnetic domains configurations }
    \label{fig:skyrmions_textures}
\end{figure}

%1.2. Relevancia aplicada: por qué las nanopartículas magnéticas importan

Magnetic nanoparticles and nanodots possess a unique versatility that makes them
relevant across multiple fields:  
a) Spintronics and Non-Volatile Magnetic Memories. Topologically protected magnetic textures, such as skyrmions, enable the storage of information as stable and switchable bits. These structures support the development of MRAM, racetrack memories, and magnetic logic devices, combining high density, low power consumption, and radiation resistance \cite{Morshed2021, Sisodia2022, Zhang2015, Eurek2024}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/mram.png}
    \caption{ Magnetic Random Memories }
    \label{fig:skyrmions_textures}
\end{figure}


b) Magnetic nanoparticles are used in oncological hyperthermia therapies, drug targeting, and enhanced magnetic resonance imaging (MRI). Their efficiency depends on the dynamic spin response under alternating fields, which is determined by the internal structure and anisotropy of the material \cite{Barrera2023, IronOxideReview2023, MLDesign2023}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/medicine.jpg}
    \caption{ Biomedical applications }
    \label{fig:skyrmions_textures}
\end{figure}

c) Energy Conversion and Storage. In the energy field, magnetic nanoparticles are
investigated for magnetocaloric refrigeration, thermoelectric conversion, and magnetic
energy storage \cite{He2025, Nehan2024, Hadouch2022}. The spin configurations determine
the mechanisms of energy transfer between magnetic and thermal degrees of freedom,
directly influencing the efficiency of these processes \cite{Liedienov2021, Nehan2024}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/storage.png}
    \caption{ Biomedical applications }
    \label{fig:skyrmions_textures}
\end{figure}





%2.4 Scientific Importance of Nanoscale Studies

The analysis of magnetic systems at the nanoscale not only drives the development of new technologies but also contributes fundamental knowledge about emergent phenomena in condensed matter physics \cite{Xiao2023, Liao2023}. These systems serve as natural models of complex physical behavior, where local interactions give rise to collective responses that challenge classical linear approximations \cite{Xiao2023, DeepSPIN2023}.

In recent years, the integration of Artificial Intelligence (AI) with computational physics has opened new avenues for the modeling and prediction of complex material behaviors \cite{Hilgers2025, Ryu2025, Goswami2022}. Traditional simulation methods offer precise control over physical parameters but are limited by high computational cost and restricted scalability across parameter spaces \cite{Liao2023}. AI-driven approaches, particularly deep learning architectures, enable the extraction of latent physical relationships directly from data, accelerating the exploration of vast configuration spaces that would otherwise be computationally intractable \cite{Xiao2023, Hilgers2025}. When coupled with physics-informed principles, these models not only enhance predictive accuracy but also preserve interpretability and physical consistency \cite{Ryu2025, Sharma2023}. In the context of nanoscale magnetism, such hybrid frameworks bridge the gap between experimental observation and theoretical modeling, providing a data-efficient path toward inverse design and the discovery of emergent spin phenomena \cite{DeepSPIN2023, Liao2023}.


\vspace{0.5cm}
%%% PARRAFO CON RELACIÓN A GCPDS
%%%
%%% REFERENCIAS DEL GRUPO?
%%%
This work is developed within the Research Signal Processing and Recognition  (GCPDS), which has extensive experience in the design and implementation of intelligent systems for signal and image analysis. The group has explored artificial intelligence methodologies across multiple domains, including image processing, temporal series analysis, and, more recently, electroencephalographic (EEG) signal interpretation. Building on this background, GCPDS is currently expanding its research toward the application of AI in computational physics, focusing on the intersection between data-driven learning and physical modeling. Current efforts involve the integration of generative models, interpretability mechanisms, transformer-based architectures, and optimization strategies to develop hybrid frameworks capable of revealing underlying physical principles from complex experimental data. This interdisciplinary approach establishes a foundation for applying advanced machine learning to the understanding and design of magnetic systems at the nanoscale. %% PAULINA


\section{Problem Statement}
%%%% Experimental and Theoretical Approaches to Magnetic Nanostructures
 
The study of magnetic nanostructures integrates two complementary approaches: the \textbf{experimental} and the \textbf{theoretical-simulation} pathways \cite{Streubel2022}. The experimental route focuses on the physical synthesis, visualization, and measurement of magnetic systems \cite{Zhang2024MFM, Wiesendanger2022SPSTM}, whereas the theoretical approach aims to model and predict their magnetic behavior from first principles or phenomenological descriptions \cite{Evans2014Atomistic, MuMaxReview2021}.


From the experimental perspective, research begins with the \textit{sample preparation} of thin films, multilayers, or nanostructures that define the system’s geometry and magnetic anisotropy \cite{Nagaosa2021Deposition, Fert2022ThinFilms}. These samples are then characterized through advanced \textit{magnetic imaging techniques} such as Lorentz Transmission Electron Microscopy (TEM), Magnetic Force Microscopy (MFM), and Kerr microscopy, which reveal the domain patterns at nanometer resolution \cite{Streubel2022, Zhang2024MFM, McVitie2023Lorentz}. The resulting data are complemented by \textit{acquisition and analysis} of external stimuli, including magnetic and electric fields (STT and SOT effects), strain fields, and temperature variations, which allow for the control and manipulation of domain structures \cite{Manchon2019SOT, Yu2022StrainControl, Finco2024Stimuli}.


In parallel, the theoretical and computational approach seeks to describe these systems at different scales. At the \textit{atomistic level}, models such as the Heisenberg Hamiltonian and Monte Carlo simulations capture spin interactions with atomic precision \cite{Evans2014Atomistic, Nowak2007MC, Hinzke2022Atomistic}. Moving to larger scales, \textit{micromagnetic simulations} employ the Landau–Lifshitz–Gilbert (LLG) equation to model the continuous magnetization field, implemented through frameworks like MuMax3, OOMMF, and Spirit \cite{MuMaxReview2021, Donahue1999OOMMF, Spirit2020}.

Together, these methodologies constitute a multi-scale, multi-modal ecosystem that bridges empirical observation and theoretical understanding, providing the foundation for modern research in magnetic materials \cite{Gomez2023MultiscaleReview}.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/ProblemStatement.png}
    \caption{ Magnetic Domain Approach }
    \label{fig:skyrmions_textures}
\end{figure}

From an experimental perspective, advances in Magnetic Force Microscopy (MFM) and Spin-Polarized Scanning Tunneling Microscopy (SP-STM) have enabled direct visualization of nanoscale magnetic textures, revealing complex domain morphologies such as vortices, skyrmions, and labyrinthine patterns \cite{Zhang2024MFM, Wiesendanger2022SPSTM, McVitie2023Lorentz}. However, experimental images often contain noise, contrast distortions, and limited resolution, making quantitative parameter extraction challenging \cite{Gao2021NoiseMFM, Phark2022ResolutionLimits}. As a result, bridging the gap between simulated and observed magnetic configurations remains an open problem that hinders both predictive modeling and materials discovery \cite{Liao2023, DeepSPIN2023, Finco2024Stimuli}.


%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%% EXTENSIÓN PARA HABLAR DE LAS SIMULACIONES Y PROBLEMAS 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Despite the significant advances in atomistic and micromagnetic simulations, the theoretical–computational approach faces several inherent limitations that restrict its scalability and practical applicability. First, the computational cost associated with solving high-dimensional Hamiltonian systems grows rapidly with the number of spins and spatial resolution, making large-scale or time-dependent studies computationally prohibitive \cite{Gomes2022Limitations, Hinzke2022Atomistic}. Parameter sweeps over exchange coupling, anisotropy, or Dzyaloshinskii–Moriya interaction (DMI) often require thousands of independent simulation runs, each demanding considerable processing time and memory resources \cite{Gomez2023MultiscaleReview, MuMaxReview2021}, and the approximation level of each simulation method imposes constraints on physical fidelity \cite{Nowak2007MC, Evans2014Atomistic}. Atomistic models offer high precision but are limited to small systems, while micromagnetic models extend spatial coverage at the expense of losing atomistic detail \cite{Spirit2020, McMichael2009Limitations}. Incorporating temperature effects, stochastic fluctuations, and material imperfections further increases complexity and reduces interpretability \cite{Hinzke2022Atomistic, Brown1963Thermal}.  



%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% INTRODUCCIÓN AL PROBLEMA INVERSO Y DIRECTO %
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
Within this integrated context, the study of magnetic nanostructures can be framed around two complementary and interdependent challenges: the direct and the inverse magnetic domain problems \cite{Feng2024, Liao2023}.

The direct problem involves predicting or generating the magnetic domain configuration that emerges from a given set of Hamiltonian parameters—exchange interaction, anisotropy, DMI, external magnetic field, and temperature. This process defines the forward mapping from the parameter space to the spatial magnetization distribution and is typically addressed using micromagnetic or atomistic simulations \cite{Evans2014Atomistic, MuMaxReview2021, Nowak2007MC}. However, such simulations are often computationally intractable for broad parameter exploration or for incorporating stochastic thermal and experimental effects \cite{Gomes2022Limitations, Hinzke2022Atomistic, Brown1963Thermal}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Direct.png}
    \caption{ Magnetic Domain Approach }
    \label{fig:skyrmions_textures}
\end{figure}

In contrast, the inverse problem aims to infer the underlying physical parameters responsible for a given observed domain configuration, such as those obtained from Magnetic Force Microscopy (MFM) or Spin-Polarized Scanning Tunneling Microscopy (SP-STM) \cite{Zhang2024MFM, Wiesendanger2022SPSTM}. This task is intrinsically ill-posed: multiple parameter combinations can reproduce visually similar textures, leading to degeneracy and uncertainty in parameter estimation \cite{Feng2024, Liu2023, DeepSPIN2023}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Inverse.png}
    \caption{ Magnetic Domain Approach }
    \label{fig:skyrmions_textures}
\end{figure}

Bridging these two directions—ensuring coherence between generation (direct mapping) and estimation (inverse mapping)—is essential for predictive materials discovery \cite{Feng2024, Park2024}. A unified and physically consistent framework that can simultaneously learn both directions offers a pathway toward explainable, data-driven modeling of nanoscale magnetic phenomena \cite{DeepSPIN2023, Ryu2025, Hilgers2025}.

Overall, the study of magnetic nanodots is constrained by three fundamental challenges that motivate the development of hybrid approaches combining physical modeling and data-driven inference \cite{Gomez2023MultiscaleReview, Liao2023, Evans2014Atomistic}.



\begin{figure}[H]
\centering
\begin{adjustbox}{center, max width=\textwidth}
\begin{tikzpicture}[
  box/.style = {draw, rounded corners=6pt, minimum width=5cm, minimum height=1.6cm, align=center, font=\small},
  process/.style = {draw, rounded corners=6pt, minimum width=5cm, minimum height=1.4cm, align=center, font=\footnotesize},
  arr/.style = {-{Latex[length=3mm,width=2mm]}, line width=0.9pt},
  dashedarr/.style = {->, dashed, line width=0.9pt},
  node distance=12mm
]

% --- Nodes (top to bottom) ---
\node[box] (ham) {Hamiltonian parameters\\[2pt]
\footnotesize $J$ (Exchange), $K$ (Anisotropy),\\ $D$ (DMI), $H$ (Field), $T$ (Temperature)};

\node[process, below=of ham] (direct) {Direct model / Simulation\\
\footnotesize Energy minimization \\ Monte Carlo / LLG / Diffusion generator};

\node[box, below=of direct] (domain) {Domain configuration\\[2pt]
\footnotesize (MFM / SP-STM-like texture: vortices, skyrmions, stripes)};

\node[process, below=of domain] (inverse) {Inverse model \\ \footnotesize Feature extraction $\rightarrow$ Bayesian / NN regression};

\node[box, below=of inverse] (est) {Estimated Hamiltonian parameters};

% --- Arrows ---
\draw[arr] (ham) -- (direct);
\draw[arr] (direct) -- (domain);
\draw[arr] (domain) -- (inverse);
\draw[arr] (inverse) -- (est);

% --- Loop arrow back up (cycle consistency) ---
\draw[arr] (est.east) .. controls +(2.5,0) and +(2.5,0) .. (ham.east)
node[midway, right=1mm, font=\footnotesize, align=center] {Cycle-consistency\\(reconstruction / energy loss)};

% --- Labels ---
\node[font=\footnotesize, align=center, left=3mm of direct] {Direct mapping\\(forward problem)};
\node[font=\footnotesize, align=center, left=3mm of inverse] {Inverse mapping\\(backward problem)};

\end{tikzpicture}
\end{adjustbox}
\caption{Vertical unified cycle illustrating the direct (top-down) and inverse (bottom-up) problems in magnetic domain research. The cycle-consistency loop (right) enforces physical coherence between estimated Hamiltonian parameters and generated magnetic textures.}
\label{fig:vertical-cycle}
\end{figure}



Overall, the study of magnetic nanodots is constrained by three fundamental challenges that motivate the development of hybrid approaches combining physical modeling and data-driven inference:

\vspace{0.5cm}

\subsection{Degeneracy of Magnetic States}


\vspace{0.5cm}

Different combinations of Hamiltonian parameters—such as exchange interaction, anisotropy, and Dzyaloshinskii–Moriya interaction (DMI)—can lead to visually similar magnetic patterns \cite{Feng2024, Liao2023, DeepSPIN2023}. This degeneracy complicates the unique identification of physical parameters from experimental observations, creating ambiguity in inverse estimation tasks \cite{Park2024, Liu2023InverseML, Raju2022SkyrmionInverse}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/Problems1.png}
    \caption{Degenerate magnetic states arising from distinct parameter combinations.}
    \label{fig:degeneracy_problem}
\end{figure}

In nanoscale magnetic systems, the relationship between the Hamiltonian parameters and the resulting domain configurations is highly nonlinear and often degenerate \cite{Feng2024, Liao2023}. This degeneracy arises because different combinations of exchange coupling (J), magnetic anisotropy (K), and Dzyaloshinskii–Moriya interaction (D) can produce equilibrium states with similar total energy and comparable spatial magnetization patterns \cite{Raju2022SkyrmionInverse, DeepSPIN2023}. From a physical perspective, the energy landscape of the magnetic Hamiltonian contains multiple local minima corresponding to metastable spin textures—such as vortices, skyrmions, or labyrinthine domains—that can be stabilized under different parameter sets \cite{Leonov2020EnergyLandscape, Du2022MetaStableSkyrmions}. In practice, this means that small compensations between parameters can preserve the overall balance of competing interactions: for example, an increase in anisotropy can be offset by a stronger exchange term or a weaker DMI, yielding morphologically indistinguishable configurations \cite{Liu2023InverseML, Bessarab2018EnergyBarriers}. Thermal fluctuations and boundary effects further blur these distinctions by allowing transitions between nearby minima, effectively broadening the range of parameter combinations that generate similar observable textures \cite{Hertel2023Thermal, Maletta2021BoundaryEffects}. As a result, the mapping from physical parameters to domain patterns is non-injective, complicating the inverse identification of Hamiltonian parameters from experimental images and introducing intrinsic ambiguity in inverse modeling tasks \cite{Park2024, Ryu2025}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/NonUniqueness.png}
    \caption{Energy Landscape of magnetic hamiltonian}
    \label{fig:degeneracy_problem}
\end{figure}

In addition to energy-based degeneracy, a second source of non-uniqueness stems from the geometric symmetries intrinsic to many spin Hamiltonians. For systems where the magnetic energy is invariant (or nearly invariant) under in-plane rotations and translations, distinct domain configurations obtained by rotating or shifting the spin texture may represent the same physical state. Recent works exploiting equivariant neural-network architectures have shown that incorporating these Euclidean symmetries (rotational and translational) into the learning model significantly improves generalization and ensures physically consistent predictions across symmetry-related configurations \cite{Zhong2023EquivariantMagnetic, Yuan2024MagNet}. Consequently, image-based states differing only by rigid motions should be considered equivalent under the symmetry group, adding a geometric layer to the degeneracy beyond parameter compensation.


For data-driven approaches to forward (parameters → texture) or inverse (texture → parameters) mapping, neglecting geometric degeneracy can lead to spurious variability: models may reproduce correct energies but output textures that are rotated or translated versions of the 'reference', or misassign parameters to physically equivalent but geometrically different inputs. Equivariant models (i.e., models whose architecture respects the underlying symmetry group) or preprocessing steps that canonicalize orientation/position of textures have been demonstrated to significantly reduce such artifacts while preserving physical fidelity \cite{Miyazaki2023EquivariantSpinNN, Zhong2023EquivariantMagnetic}. Therefore, explicitly accounting for geometric symmetries is crucial to disentangle true degeneracy in parameters from trivial degeneracy due to spatial transformations.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.8\textwidth]{Figures/GeometricDegeneracy.png}
    \caption{Geometric Degeneracy Effect}
    \label{fig:degeneracy_problem}
\end{figure}

\subsection{Low Quality Data}
Magnetic domain imaging and modeling face intrinsic limitations that affect the quality
and reliability of observable data. On the experimental side, characterization techniques
such as Magnetic Force Microscopy (MFM) and Spin-Polarized Scanning Tunneling
Microscopy (SP-STM) provide direct access to nanoscale magnetic textures but are
fundamentally constrained by instrumental noise, finite probe resolution, and contrast
distortions \cite{Schlenhoff2022SPSTM, Kramer2021MFMNoise, Raabe2020ImagingLimits}. These imperfections obscure the fine spatial structure of the magnetization
field, complicating the quantitative extraction of physical parameters and domain wall
profiles \cite{Hanneken2022ResolutionLimit, Wiesendanger2023SPSTMReview}. On the computational side, numerical simulations—whether atomistic or
micromagnetic—introduce their own forms of degradation. Discretization errors in
the mesh, finite time-step integration of the Landau–Lifshitz–Gilbert (LLG) dynamics,
and simplifications in boundary or anisotropy conditions limit the accuracy of the
resulting spin configurations \cite{Evans2014Atomistic, Exl2021NumericalErrors, Hertel2022MicromagneticLimits}. As a result, both experimental and simulated datasets
often represent only approximations of the true magnetic state, making the comparison
between observed and modeled textures inherently uncertain \cite{Liao2023, Park2024}.


\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/Lab.png}
    \caption{Experimental observation limitations due to noise and resolution constraints.}
    \label{fig:experimental_limitations}
\end{figure}

\begin{table}[H]
\centering
\caption{Approximate operational costs and limitations of magnetic imaging techniques.}
\label{tab:experimental_costs_monetary}
\renewcommand{\arraystretch}{2}
\large
\resizebox{\textwidth}{!}{
\begin{tabular}{lcccc}
\hline
\textbf{Technique} & \textbf{Spatial Resolution} & \textbf{Main Limitations} & \textbf{Quantitative Accuracy} & \textbf{Estimated Cost (USD/hour)} \\ \hline
Magnetic Force Microscopy (MFM) & 30–50 nm & Tip-induced artifacts, long scan time & Moderate & 150–300 \\
Spin-Polarized STM (SP-STM) & $\sim$1 nm & Requires UHV and cryogenic conditions & High (atomic-level) & 800–1500 \\
Lorentz Transmission Electron Microscopy (Lorentz TEM) & 5–10 nm & Complex sample prep, projection effects & Moderate–High & 400–800 \\
Kerr Microscopy & 300–500 nm & Optical diffraction limit, surface sensitivity & Low–Moderate & 100–250 \\
X-ray Magnetic Circular Dichroism (XMCD) & 10–30 nm & Requires synchrotron access & High & 2000–5000 \\
Electron Holography & 2–5 nm & High vacuum, phase noise & High & 500–1000 \\ \hline
\end{tabular}
}
\end{table}


From a physical standpoint, this degradation is a direct manifestation of the multi-
scale and metastable nature of magnetic systems. The magnetic free-energy landscape
is rugged, containing multiple nearby minima separated by small energy barriers \cite{Leonov2020EnergyLandscape, Bessarab2018EnergyBarriers}. Even
slight variations in temperature, defects, or external fields can cause the system to
relax into different local minima that are energetically similar but structurally distinct \cite{Du2022MetaStableSkyrmions, Hertel2023Thermal}. Consequently, any measurement or simulation captures only one realization of a broader
ensemble of possible states. In experimental observations, this variability is further
blurred by the indirect nature of detection: instruments like MFM record the stray field
above the surface, which is a convolution of the probe’s sensitivity and the sample’s
magnetization, rather than a direct mapping of the spin structure itself \cite{Kramer2021MFMNoise, Raabe2020ImagingLimits}. Similarly, in
simulations, numerical smoothing and discretization coarsen the magnetization field,
suppressing high-frequency details \cite{Exl2021NumericalErrors, Hertel2022MicromagneticLimits}. Together, these effects impose a fundamental limit
on the fidelity of both experimental and computational data, constraining the precision
of parameter estimation and the interpretability of magnetic textures \cite{Liao2023, Park2024}.



\begin{figure}[H]
    \centering
    \includegraphics[width=0.97\textwidth]{Figures/Sensitive.png}
    \caption{Experimental observation limitations due to noise and resolution constraints.}
    \label{fig:experimental_limitations}
\end{figure}

\subsection{Lack of physic-based interpretability}
A persistent challenge in data-driven modeling of magnetic systems is the lack of in-
terpretability and physical traceability in the results. While modern machine learning
methods can efficiently approximate the complex mapping between Hamiltonian pa-
rameters and magnetic domain configurations, their internal representations are often
opaque, providing limited insight into the underlying physics that govern the observed
phenomena \cite{Rao2022ExplainableMaterialsML, Park2024, DeepSPIN2023}. This opacity complicates the validation of learned relationships against es-
tablished physical laws, such as exchange symmetry, anisotropy effects, or DMI-induced
chirality \cite{Chen2021PINNMagnetism, Ryu2025, Schneider2022MLMagnetism}. Furthermore, when combined with noisy experimental data or degenerate
parameter spaces, black-box models may produce accurate reconstructions without
preserving causal or physically meaningful dependencies \cite{Liao2023, Liu2023InverseML}. Consequently, the absence
of interpretability not only limits the scientific reliability of such approaches but also
hinders their use in guiding material design and hypothesis generation \cite{Butler2018MLMaterials, Sanchez2023ExplainableDesign}. Bridging this
gap requires the integration of physics-informed constraints, explainable architectures,
and cycle-consistent frameworks that ensure alignment between learned representations
and physically observable quantities \cite{Karniadakis2021PINNs, Sanchez2024CycleConsistentMaterials}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.45\textwidth]{Figures/Interpretability.png}
    \caption{Interpretability Challenges}
    \label{fig:experimental_limitations}
\end{figure}

The three outlined challenges---parameter degeneracy, data degradation, and lack of interpretability---are deeply interconnected within the study of magnetic domain systems \cite{Feng2024, Exl2021NumericalErrors, Rao2022ExplainableMaterialsML}. Parameter degeneracy reflects the intrinsic non-uniqueness of the physical mapping between Hamiltonian parameters and magnetic configurations, producing multiple metastable states with similar energy and morphology \cite{Leonov2020EnergyLandscape, Du2022MetaStableSkyrmions}. Data degradation, in turn, limits the quality and reliability of both experimental and simulated observations, as noise, instrumental artifacts, and numerical discretization blur the underlying spin structures \cite{Kramer2021MFMNoise, Exl2021NumericalErrors, Raabe2020ImagingLimits}. Finally, the lack of interpretability in data-driven models prevents the extraction of causal or physically meaningful relationships from these imperfect datasets, reducing their scientific value and applicability \cite{Park2024, Chen2021PINNMagnetism, Sanchez2023ExplainableDesign}. Together, these issues define a complex landscape where conventional simulation-based approaches struggle with scalability and uncertainty, while purely data-driven models risk overfitting and opacity \cite{Gomez2023MultiscaleReview, Butler2018MLMaterials, Ryu2025}. This convergence of physical ambiguity, informational degradation, and epistemic opacity motivates the development of hybrid, physics-informed frameworks capable of enforcing physical consistency while enabling explainable inference across the direct and inverse magnetic domain problems \cite{Karniadakis2021PINNs, DeepSPIN2023, Sanchez2024CycleConsistentMaterials}.

\bigskip




%


%



%
\section{State of the art}

\noindent
The study of magnetic domain systems has evolved through complementary experimen-
tal, theoretical, and computational approaches, each contributing to the understanding
of spin textures and their emergent phenomena \cite{Wiesendanger2023SPSTMReview, Gomez2023MultiscaleReview}. Recent advances in high-resolution
imaging techniques, micromagnetic simulations, and machine learning frameworks have
expanded the capacity to visualize, model, and predict complex domain morphologies
such as skyrmions, vortices, and labyrinthine structures \cite{Hanneken2022ResolutionLimit, Hertel2022MicromagneticLimits, DeepSPIN2023}. However, despite the increasing
sophistication of these methodologies, several fundamental challenges persist that con-
strain the reliability, interpretability, and generalization of current models \cite{Feng2024, Exl2021NumericalErrors, Rao2022ExplainableMaterialsML}. Specifically,
three major issues remain at the core of contemporary research: (i) the degeneracy of
the Hamiltonian parameter space, which complicates inverse estimation and physical
uniqueness \cite{Liao2023, Raju2022SkyrmionInverse}; (ii) the degradation of data quality in both experimental and simulated
observations, which limits quantitative analysis \cite{Kramer2021MFMNoise, Raabe2020ImagingLimits}; and (iii) the lack of interpretability in
data-driven approaches, which obscures causal understanding and hinders physics-based
validation \cite{Park2024, Sanchez2023ExplainableDesign}. The following subsections review the main advances and limitations related
to each of these challenges, outlining the current research gaps that motivate the present
work.



\subsection{Degeneracy of Magnetic States}


The degeneracy of magnetic states represents one of the most persistent obstacles in the modeling, interpretation, and inverse reconstruction of spin textures. This phenomenon arises because the magnetic free-energy landscape is intrinsically multimodal, admitting a large number of metastable solutions whose morphology can be remarkably similar even when generated by substantially different Hamiltonian parameters \cite{Leonov2020EnergyLandscape, Bessarab2018EnergyBarriers}. As a consequence, both the direct problem (mapping parameters to imaging contrast) and the inverse problem (inferring parameters from an observed texture) are fundamentally ill-posed. The state of the art has therefore evolved along two complementary lines of work: efforts to make the forward mapping more discriminative and physically grounded, and efforts to make the inverse mapping more expressive, uncertainty-aware, and physically constrained. What follows is a synthesis of the major developments, their strengths and limitations, and the structural gaps that motivate new approaches.
Research on the direct problem has traditionally relied on high-fidelity micromagnetic or atomistic simulations capable of capturing the subtle competition between exchange, anisotropy, DMI, dipolar interactions, and applied fields. Tools such as MuMax3, OOMMF, and Spirit have enabled precise modeling of domain walls, skyrmions, vortices, and labyrinthine patterns under a wide range of physical conditions \cite{MuMaxReview2021, Spirit2020}. These approaches are widely used in both micrometer-scale and nanoscale magnetism, particularly in the study of skyrmions, chiral domain walls, and confined spin textures in ultrathin films. When coupled with forward-rendering models that explicitly account for instrument response—such as the point-spread function of MFM, the tunneling convolution kernel in SP-STM, or noise induced by probe electronics—these simulators generate synthetic images that closely resemble their experimental counterparts \cite{Kramer2021MFMNoise, Phark2022ResolutionLimits}. This integration has been essential in applications such as skyrmion detection, micromagnetic parameter inference, and interpretation of high-resolution imaging data. Such physically accurate models substantially reduce non-physical degeneracies arising from oversimplified imaging assumptions.
Nevertheless, these simulator-based approaches are computationally expensive and sensitive to numerical discretization, initialization conditions, and solver choices, all of which can artificially inflate the observed degeneracy. More importantly, even perfect numerical solvers cannot eliminate intrinsic degeneracy: distinct combinations of DMI, anisotropy, exchange, and dipolar strengths may legitimately yield nearly indistinguishable patterns. Thus, while forward simulations reveal the structure of degeneracy, they cannot resolve it.
To mitigate these limitations, recent work has turned to differentiable and surrogate forward models. Neural surrogates, variational approximators, and differentiable micromagnetic solvers offer orders-of-magnitude speedups and enable gradient-based calibration and end-to-end integration into machine-learning frameworks \cite{Kwon2020, DeepSPIN2023}. These methods have been applied in problems such as rapid skyrmion-profile generation, uncertainty-aware forward prediction, and coarse-to-fine parameter scanning. While these models enable exploration of high-dimensional parameter spaces that are intractable for brute-force simulations, their accuracy is fundamentally limited by the diversity and physical realism of the training set. As a result, surrogate models may hallucinate non-physical spin textures or fail to generalize outside the training manifold. Consequently, this research line—although actively growing—is not yet considered a mature or commonly adopted tool for nanoscale domain reconstruction, highlighting the field’s lack of physics-faithful, constraint-preserving differentiable solvers.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/Surrogate.png}
    \caption{Neural networks Baseline}
    \label{fig:experimental_limitations}
\end{figure}. 


In parallel, physics-informed neural networks (PINNs) and hybrid modeling frameworks have gained traction by embedding the Landau–Lifshitz–Gilbert (LLG) equation or energetic constraints directly into the learning objective \cite{Karniadakis2021PINNs, Chen2021PINNMagnetism}. These approaches have been tested in applications such as reconstructing domain-wall dynamics, modeling low-dimensional magnetic textures, or enforcing physical plausibility in neural surrogate outputs. By injecting physical laws into the learning process, they help reduce the feasible set of solutions and thereby restrict degeneracy. However, PINNs for magnetic systems remain difficult to train due to the stiffness of the LLG equation, challenges in balancing data fidelity and physics terms, and poor scalability to realistic thin-film geometries and finite-temperature regimes. Their use in nanoscale magnetic domain reconstruction remains exploratory rather than widely adopted.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.75\textwidth]{Figures/PINNs.png}
    \caption{PINNs Baseline}
    \label{fig:experimental_limitations}
\end{figure}


The objective in using Deep Learning for the Forward Mapping (Surrogate Model) in micromagnetics is to construct a function that rapidly maps a low-dimensional input vector of Hamiltonian parameters to the resulting high-dimensional spin configuration image, bypassing slow micromagnetic simulations. Given a vector of $P$ physical parameters $\mathbf{y}_n \in \mathbb{R}^{P}$ (e.g., $D$, $K$, $J$, $H_{\text{ext}}$), the task is to predict the image $\mathbf{\hat{I}}_n \in \mathbb{R}^{H \times \tilde{W} \times C}$. The core of the standard Deep Learning approach is to construct a deterministic function $\mathbf{\hat{I}}_n = f_{\Theta}(\mathbf{y}_n)$, where $f_{\Theta} : \mathbb{R}^{P} \to \mathbb{R}^{H \times \tilde{W} \times C}$ is parameterized by a Deep Neural Network (DNN), such as a Generative Adversarial Network (GAN) or an up-sampling network (like a transposed CNN or U-Net variant). The architecture uses dense layers to process the input vector $\mathbf{y}_n$ into an initial feature map, followed by transposed convolutions ($\otimes^T$) to increase the spatial resolution:

$$
\mathbf{F}_l = \sigma_l(\mathbf{W}_l \otimes^T \mathbf{F}_{l-1} + \mathbf{b}_l)
$$

The final layers project this embedding into the target image space, $\mathbf{\hat{I}}_n$. The trainable parameter set $\Theta$ is optimized by minimizing the expected loss between the ground-truth spin configuration $\mathbf{I}_n$ (from high-fidelity simulations) and the prediction $\mathbf{\hat{I}}_n$:
$$
\Theta^{*} = \arg \min_{\Theta} \mathbb{E}[\mathcal{L}(\mathbf{I}_n, \mathbf{\hat{I}}_n | \Theta) : \forall n \in \mathbb{N}]
$$
where the common regression loss function $\mathcal{L}$ is the Mean Squared Error (MSE):
$$
\mathcal{L}_{\text{MSE}} = \frac{1}{N_{\text{data}} H \tilde{W} C} \sum_{n=1}^{N_{\text{data}}} || \mathbf{I}_n - \mathbf{\hat{I}}_n ||^2
$$
The critical flaw of this purely data-driven approach is its lack of physical guarantees and its inability to capture the inherent multimodality of magnetic states. Since it aims for a single deterministic output $\mathbf{\hat{I}}_n$ for a given input $\mathbf{y}_n$, it struggles to maintain physical consistency and may yield non-physical textures outside the training distribution, a fundamental problem in tackling degeneracy.

Physics-Informed Neural Networks (PINNs) resolve this by merging the data-driven loss with physical constraints. The key is to enforce that the predicted magnetization field $\mathbf{M}_{\text{NN}}$ (which corresponds to $\mathbf{\hat{I}}_n$) must satisfy the Landau--Lifshitz--Gilbert (LLG) equation, which governs the dynamics. The LLG equation is incorporated through its residual $\mathbf{R}_{\text{LLG}}(\mathbf{x}, t)$, where the necessary spatial and temporal derivatives are calculated using automatic differentiation on the NN's output:
$$
\mathbf{R}_{\text{LLG}}(\mathbf{x}, t) = \frac{\partial \mathbf{M}}{\partial t} + \frac{\gamma}{1+\alpha^2} (\mathbf{M} \times \mathbf{H}_{\text{eff}}) + \frac{\gamma \alpha}{1+\alpha^2} \left( \mathbf{M} \times (\mathbf{M} \times \mathbf{H}_{\text{eff}}) \right) = \mathbf{0}
$$
The PINN modifies the optimization objective by adding a Physics Loss ($\mathcal{L}_{\text{physics}}$) and a Magnitude Constraint Loss ($\mathcal{L}_{\text{magnitude}}$) to the total function:
$$
\mathcal{L} = \mathcal{L}_{\text{data}} + \lambda_{\text{physics}} \mathcal{L}_{\text{physics}} + \lambda_{\text{magnitude}} \mathcal{L}_{\text{magnitude}}
$$
where $\mathcal{L}_{\text{physics}}$ penalizes deviations from the LLG equation across collocation points $\mathcal{P}$:
$$
\mathcal{L}_{\text{physics}} = \frac{1}{|\mathcal{P}|} \sum_{(\mathbf{x}_i, t_i) \in \mathcal{P}} || \mathbf{R}_{\text{LLG}}(\mathbf{x}_i, t_i) ||^2
$$
And $\mathcal{L}_{\text{magnitude}}$ enforces that the magnitude remains constant ($||\mathbf{M}|| = M_s$):
$$
\mathcal{L}_{\text{magnitude}} = \frac{1}{|\mathcal{P}|} \sum_{(\mathbf{x}_i, t_i) \in \mathcal{P}} \left( || \mathbf{M}_{\text{NN}}(\mathbf{x}_i, t_i) ||^2 - M_s^2 \right)^2
$$
By minimizing this physics-informed loss, the network is constrained to produce outputs that are not only statistically accurate but also physically plausible, thereby reducing the non-physical solution space and offering a more robust approach to mitigating degeneracy.




A qualitatively different perspective has emerged with the use of conditional generative models—including conditional VAEs, conditional GANs, and more recently diffusion-based conditional generators. These models treat the forward mapping as a conditional probability distribution instead of a deterministic function, enabling explicit representation of multimodality \cite{Ronne2024, Alverson2024, Chen2024}. Such techniques have been used primarily in large-scale imaging tasks, inverse scattering, and materials microstructure synthesis, with only very recent work beginning to apply them to magnetic textures, particularly in simulating ensembles of metastable states or producing uncertainty-aware synthetic datasets. Their ability to sample multiple plausible outputs for a given parameter set directly reflects the inherent degeneracy of magnetic states. However, generative models lack physical guarantees unless combined with physics-based constraints or energy-informed discriminators, and their outputs may deviate significantly from micromagnetic ground truth. To date, they remain promising but immature tools within nanoscale magnetism, with substantial gaps in validation, benchmarking, and physical consistency.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/cVae.png}
    \caption{CVAE Baseline}
    \label{fig:experimental_limitations}
\end{figure}

\begin{table}[H]
\centering
\footnotesize
\caption{Summary of State-of-the-Art Approaches for Modeling Magnetic State Degeneracy}
\label{tab:sota_magnetism_improved}
\begin{tabularx}{\textwidth}{@{} p{3.2cm} | p{2.2cm} | X | X @{}}
\toprule
\textbf{Research Line} & \textbf{Key Tools} & \textbf{Strengths} & \textbf{Limitations} \\
\midrule
\textbf{High-Fidelity Forward Simulation} & MuMax3, OOMMF, Spirit &
Physical ground truth; reliable synthetic data generation &
Computationally expensive; reveals but cannot resolve degeneracy \\
\midrule
\textbf{Differentiable/ Surrogate Models} & DeepSPIN, Neural Surrogates &
Orders-of-magnitude speedup; enables gradient-based optimization &
Limited generalization; risk of non-physical textures outside training domain \\
\midrule
\textbf{Physics-Informed Networks} & Hybrid LLG/Energy Frameworks &
Enforces physical consistency; reduces feasible solution set &
Training instability (stiff LLG); poor scalability to realistic geometries \\
\midrule
\textbf{Conditional Generative Models} & cVAEs, GANs, Diffusion &
Explicitly samples multimodality; treats mapping as probability distribution &
Lacks physical guarantees; immature validation in nanoscale magnetism \\
\bottomrule
\end{tabularx}
\end{table}




On the inverse side, early approaches framed parameter estimation as a supervised
regression problem, typically using convolutional or fully connected neural networks
trained on large synthetic datasets \cite{Kwon2020, Feng2024}. These models are
computationally efficient and capable of predicting parameters in real time, making
them attractive for high-throughput applications in micromagnetism, spintronics, and
magnetic materials characterization. However, strict regression collapses the inherent
ambiguity of the inverse problem into a single point estimate, making predictions highly
sensitive to sim-to-exp mismatch and dataset bias. When two or more parameter regimes
produce nearly indistinguishable images, these models tend to yield unstable or
misleading predictions. Their dependence on large representative datasets introduces a
significant bottleneck, as the quality and diversity of synthetic training data are limited
by the accuracy, coverage, and numerical stability of the forward simulator.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.85\textwidth]{Figures/SimpleEstimation.png}
    \caption{Supervised Regression}
    \label{fig:experimental_limitations}
\end{figure}


The limitations of deterministic regression have motivated a transition toward Bayesian
inference methodologies, which represent the inverse mapping as a posterior
distribution instead of a single estimate \cite{Hu2025,
PRMaterials2025_phaseboundaries}. By explicitly modeling uncertainty and
multi-modality, Bayesian methods reveal the underlying degeneracy rather than hiding
it, offering richer insights into confidence levels, plausible parameter regions, and
non-identifiability. These frameworks have gained traction in the analysis of
magnetization textures, phase boundary detection, and materials discovery.
Nevertheless, Bayesian inference remains computationally expensive because it
requires repeated forward evaluations—each often involving a full micromagnetic
simulation or a surrogate model. Posterior accuracy also depends critically on the form
of the likelihood and prior, which are difficult to specify for systems governed by the
Landau–Lifshitz–Gilbert equation and subject to complex magnetic interactions.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.73\textwidth]{Figures/Bayesian.png}
    \caption{Bayesian Inference}
    \label{fig:experimental_limitations}
\end{figure}

To address these challenges, cycle-consistent frameworks have recently emerged as a
promising direction. These approaches couple a forward surrogate with an inverse
estimator, enforcing that both are consistent with one another through a cyclic
constraint \cite{DeepSPIN2023, Sanchez2024CycleConsistentMaterials}. By requiring
that inferred parameters can regenerate the original image via the forward model, these
methods naturally filter out many non-physical or inconsistent solutions. They have
shown potential in magnetic imaging analysis, ferromagnetic resonance inversion, and
domain morphology interpretation. However, cycle-consistent schemes inherit all
biases from the forward surrogate; if the surrogate is inaccurate or poorly calibrated,
the cycle constraint may amplify—rather than mitigate—systematic errors. Moreover,
their stability when applied to micromagnetic data remains largely unexplored, and
training can become unstable in strongly degenerate regimes.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.73\textwidth]{Figures/Hybrid.png}
    \caption{Cyclic Constraint}
    \label{fig:experimental_limitations}
\end{figure}

Hybrid inverse methods represent a more recent development, combining statistical
inference with physics-informed constraints. These include Bayesian Physics-Informed
Neural Networks (PINNs), energy-regularized inverse networks, and approaches that
penalize violations of the LLG equation or magnetic free-energy minimization
\cite{Chen2021PINNMagnetism, Ryu2025}. By embedding physical priors directly in
the inference process, these methods restrict predictions to physically interpretable and
energetically plausible regions of parameter space. Such strategies have demonstrated
promise in nanoscale magnetometry, skyrmion parameter identification, and
magnetization-dynamics inversion. Their main drawbacks lie in computational cost,
training stiffness, and the difficulty of sampling complex posteriors that remain highly
multimodal in degenerate regimes.

\begin{table}[H]
\centering
\footnotesize
\caption{Summary of State-of-the-Art Inverse Modeling Approaches Under Magnetic Degeneracy}
\label{tab:sota_inverse_magnetism}
\begin{tabularx}{\textwidth}{@{} p{3.2cm} | p{2.2cm} | X | X @{}}
\toprule
\textbf{Inverse Modeling Approach} & \textbf{Key Tools / Methods} & \textbf{Strengths} & \textbf{Limitations} \\
\midrule

\textbf{Supervised Regression Models} 
& CNN/MLP regressors \cite{Kwon2020, Feng2024} 
& Ultra-fast inference; simple to implement; useful for coarse parameter screening 
& Collapse multimodality into point estimates; highly sensitive to dataset bias and sim-to-exp mismatch; unreliable under degeneracy \\

\midrule

\textbf{Bayesian Inference Frameworks} 
& MCMC, VI, Bayesian NNs \cite{Hu2025, PRMaterials2025_phaseboundaries}
& Captures uncertainty and multimodality; reveals ambiguous parameter regimes; provides posterior distributions 
& Computationally expensive; requires repeated forward simulations; results depend on priors and likelihood design \\

\midrule

\textbf{Cycle-Consistent Forward--Inverse Models} 
& DeepSPIN-like surrogates, cycle-consistent networks \cite{DeepSPIN2023, Sanchez2024CycleConsistentMaterials}
& Enforces physical consistency; filters non-physical solutions; integrates forward and inverse learning jointly 
& Inherits surrogate biases; can reinforce systematic errors; training stability in micromagnetics largely unexplored \\

\midrule

\textbf{Physics-Informed and Hybrid Inference Models} 
& Bayesian PINNs, LLG residual penalties, energy regularization \cite{Chen2021PINNMagnetism, Ryu2025}
& Embeds physics into the inverse map; restricts inference to physically valid regions; improves interpretability 
& High computational cost; stiff training dynamics; posteriors remain hard to sample in strongly degenerate regimes \\

\midrule

\textbf{Generative Inverse Models} 
& cVAEs, conditional diffusion, inverse-GANs 
& Learns full parameter distributions; naturally captures multi-modality; scalable to large datasets 
& Weak physical guarantees; unstable training; limited validation in nanoscale magnetism \\

\bottomrule
\end{tabularx}
\end{table}


Despite the progress enabled by these diverse methodologies, structural gaps remain in
the state of the art. First, there is no standardized benchmarking protocol that
systematically evaluates fidelity, uncertainty calibration, and physical consistency of
inverse models across geometries, material parameters, and imaging modalities. Second,
numerical uncertainty in micromagnetic solvers is rarely quantified or propagated into
inverse predictions, limiting the interpretability of uncertainty estimates. Third, the
absence of certified differentiable micromagnetic simulators—and the limited
understanding of surrogate-induced biases—hinders the development of reliable inverse
mappings. Finally, existing approaches seldom integrate physics constraints,
uncertainty quantification, and cycle consistency into a unified architecture capable of
handling both direct and inverse degeneracy in a principled manner. This gap motivates
the need for next-generation inverse methods that jointly incorporate physical priors,
distributional modeling, and forward–inverse structural consistency.





\subsection{Low-Quality Data}

The quality of magnetic-domain images (MFM, SP-STM, Kerr, Lorentz TEM, etc.) critically conditions both forward modeling and inverse inference. Experimental images often suffer from low signal-to-noise ratio (SNR), probe convolution, contrast distortions, finite spatial resolution and sample variability; numerical datasets produced by simulators can display different noise statistics and missing instrumental effects. These issues produce two distinct but related failure modes: (i) loss of physically informative fine structure (domain walls, skyrmion cores), which degrades parameter identifiability; and (ii) domain shift between simulation and experiment, which causes learned models to generalize poorly. The recent literature addresses these problems through a set of complementary strategies: denoising and super-resolution (including zero-shot/self-supervised methods), domain adaptation / transfer learning (sim2real), physics-informed reconstruction (PINNs and physics-regularized denoisers), instrument modelling and deconvolution, and generative priors used as reconstructors or regularizers. Below we summarize each approach, list strengths/weaknesses, and point out explicit gaps that motivate further work.

\paragraph{Denoising and super-resolution (classical and deep learning).}  
Deep denoising and single-image super-resolution methods are applied to recover fine spatial details necessary for physical interpretation (sharp domain walls, core polarity). Supervised CNNs and residual networks trained on paired synthetic noisy/clean images achieve excellent reconstruction when training data matches test conditions, while self-supervised and zero-shot methods (e.g., Noise2Void, Noise2Self, zero-shot SR) enable enhancement without paired HR/LR datasets, a crucial advantage when experimental ground truth is scarce \cite{Noise2Void2019, SelfSuper2020, PMC11230634}. The main risk is over-smoothing or creation of hallucinated features that do not correspond to actual magnetization structure; supervised models trained on synthetic physics outputs can introduce domain-specific artefacts if instrument effects are mismatched.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{Figures/ZeroShot.png}
    \caption{Denoising  }
    \label{fig:experimental_limitations}
\end{figure}


\paragraph{Domain adaptation.}  
Bridging the simulation–experiment gap is commonly addressed through domain adaptation and transfer learning: fine-tuning networks on a (small) set of real images, adversarial domain adaptation to align feature distributions, and test-time adaptation techniques that adapt model statistics on the fly. These techniques reduce the performance drop when deploying inverse regressors on experimental images, but they require at least some representative real data or reliable unsupervised adaptation heuristics; moreover, they cannot correct for unmodelled physical differences (e.g., unknown tip magnetization, unknown boundary roughness) \cite{Sim2RealReview2021, PubMed33994917}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{Figures/Domain.png}
    \caption{Domain adaptation }
    \label{fig:experimental_limitations}
\end{figure}

\paragraph{Physics-informed reconstruction (PINNs and physics-regularized denoisers).}  
Incorporating physics—either as explicit PDE residuals (LLG-based terms), as energy penalties, or via hard constraints—improves robustness under low SNR by biasing reconstructions toward physically admissible solutions. PINNs and hybrid networks can suppress implausible denoising outcomes and provide better feature stability for downstream inference \cite{Karniadakis2021PINNs, MDPI_DIAG2022}. Their drawbacks are practical: LLG residuals are stiff and expensive to evaluate, training is sensitive to loss weighting, and incomplete physical models (e.g., neglecting defects, thermal fluctuations) may introduce biases.



\paragraph{Generative priors and conditional generative models.}  
Generative priors—learned via conditional GANs, normalizing flows or (recently proposed) diffusion-based priors—encode the manifold of realistic clean textures and can act as strong regularizers during reconstruction or inverse estimation. Using a forward generative model as a plausibility evaluator or plug-in denoiser constrains solutions to plausible magnetization patterns, thereby reducing spurious parameter estimates. The main limitations are data hunger (large, varied clean datasets required), potential bias from dataset composition, and computational cost for advanced priors.


\subsection*{Concise summary table}

\begin{table}[htbp]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.65} % aumenta la altura de las filas
\begin{tabular}{p{3.0cm} p{4.0cm} p{4.0cm}}
\hline
\textbf{Strategy} & \textbf{How it helps} & \textbf{Key limitations} \\
\hline
Denoising / Super-resolution (deep) 
& Recovers fine structures and improves SNR; self-supervised and zero-shot methods allow use with scarce experimental data.
& Risk of over-smoothing or hallucination; synthetic-trained models may fail on real images; small-scale features can be lost. \\
\hline
Domain adaptation / transfer learning (sim2real)
& Aligns synthetic and experimental feature distributions, reducing performance loss of inverse models.
& Requires representative real samples; unsupervised adaptation may be unstable; cannot correct missing physics. \\
\hline
Physics-informed reconstruction (PINNs, energy priors)
& Enforces physically admissible solutions and stabilizes reconstruction under low SNR.
& Computationally expensive; sensitive loss balancing; performance depends on completeness of physical models. \\

\hline
Generative priors / conditional generative models
& Encode realistic texture manifolds and regularize inverse solutions; act as learned denoisers.
& Data-demanding; potential dataset bias; computational cost of advanced priors. \\
\hline
\end{tabular}
\caption{Condensed strategies to mitigate low-quality data in magnetic domain imaging.}
\end{table}



\subsection{Interpretability and Physical Traceability: State of the Art}

A persistent challenge in data-driven modeling of magnetic systems is the lack of interpretability and physical traceability in learned models. This limitation is not unique to magnetism: across computational physics and materials science, neural surrogates often match or surpass classical models in accuracy yet remain opaque, making it difficult to validate learned relationships against established physical laws or to extract causal insight \cite{Oviedo2022}. In nanoscale magnetism this opacity is particularly problematic, as inferred material parameters (exchange, anisotropy, DMI, external field, temperature) must be numerically precise and physically meaningful to support device design, hypothesis testing, and experimental planning.

Recent work (2020–2025) addressing interpretability in physics-informed learning can be grouped into four main directions: (1) post-hoc attribution and saliency methods adapted to physics data, (2) architectures aligned with Hamiltonian structure or physical operators, (3) cycle-consistent and energy-aware hybrid models that enforce forward--inverse coherence, and (4) probabilistic and disentangled latent models that expose uncertainty and factorized physical degrees of freedom. Below we summarize advances, strengths, limitations, and open gaps relevant to micromagnetic modeling.

\paragraph{Post-hoc attribution and local explainers.}
Classical explainability methods---such as saliency maps, occlusion tests, Gradient-weighted Class Activation Mapping (Grad-CAM), Integrated Gradients, and perturbation analyses---have been adapted to materials microstructure problems and magnetization textures \cite{Oviedo2022}. These tools highlight image regions most strongly influencing model predictions, enabling researchers to verify that regressors attend to physically meaningful structures (domain walls, skyrmion cores, vortex centers) rather than spurious artefacts or background noise. Their advantages are simplicity, model-agnostic applicability, and utility as diagnostic screening tools. However, they provide only descriptive explanations rather than causal or physically grounded insight, and attribution maps can be unstable to small perturbations or architectural changes \cite{Li2023}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{Figures/Cams.png}
    \caption{ Post-hoc attribution }
    \label{fig:experimental_limitations}
\end{figure}

\paragraph{Architectures aligned with physics.}
A growing body of work embeds physical structure directly into model architectures. Examples include Hamiltonian- and energy-preserving networks, equivariant architectures for magnetic and electronic structure, operator-aware convolutions, and graph-based models for spin lattices \cite{Mattheakis2022, Li2023, Moradi2023}. Such models improve traceability by making internal components correspond to energy terms or symmetry representations; they enable per-term energy decomposition, sensitivity analysis, and physically meaningful extrapolation. Their main limitation is the need to predefine relevant physical terms and symmetries, which can reduce flexibility when unknown or emergent interactions are present.

\paragraph{Cycle-consistency, energy penalties, and hybrid forward--inverse training.}
Cycle-consistent frameworks pair a forward operator (parameters $\rightarrow$ texture) with an inverse model (texture $\rightarrow$ parameters) and enforce reconstruction and energy-based coherence \cite{Huang2023, DeepSPIN2023}. When the forward operator is a physics-informed simulator or a calibrated differentiable surrogate, cycle consistency acts as an interpretability constraint: predicted parameters must regenerate the input texture under the forward physics model. Additional energy penalties (differences in Hamiltonian energy between predicted and observed states) further restrict non-physical solutions. These methods have been used for uncertainty quantification and robust inversion in inverse-imaging and geophysical applications, and are beginning to be adapted to micromagnetic tasks. Their main weakness remains the fidelity of the forward module: surrogate bias can be propagated and reinforced.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.80\textwidth]{Figures/Penalties.png}
    \caption{ Post-hoc attribution }
    \label{fig:experimental_limitations}
\end{figure}

\paragraph{Probabilistic models, uncertainty quantification, and disentangled latent spaces.}
Probabilistic generative models—normalizing flows, variational inference, and Bayesian neural networks—provide full posterior descriptions and explicit multimodality, making degeneracy and non-identifiability observable in the posterior landscape \cite{Wei2022, Jacobsen2022}. Disentanglement methods (e.g., $\beta$-VAE variations) have been adapted to physical fields to align latent axes with generative factors, aiding interpretability and enabling targeted interventions in latent space \cite{Jacobsen2022}. Normalizing flows and flow-based variational methods have also been integrated with iterative solvers for inverse problems to produce calibrated posteriors and actionable uncertainty estimates \cite{Wei2022}. These probabilistic approaches are computationally demanding and require careful prior and likelihood modeling, but they offer the most principled route to exposing degeneracy.





\subsubsection*{Concise comparative table}
\begin{table}[htbp]
\centering
\footnotesize
\renewcommand{\arraystretch}{1.45}
\begin{tabular}{p{3.4cm} p{4.6cm} p{4.0cm}}
\hline
\textbf{Method family} & \textbf{What it provides} & \textbf{Main limitations} \\
\hline
Post-hoc attribution (Grad-CAM, IG, occlusion) & Localizes image regions driving predictions; simple model-agnostic checks. & Does not enforce physical consistency; explanations can be unstable. \\
\hline
Physics-aligned architectures & Internal modules map to energy terms or operators → direct physical traceability. & Requires explicit physical decomposition; may miss emergent effects. \\
\hline
Cycle-consistent + energy losses & Enforces forward–inverse coherence and energy plausibility; reduces non-physical estimates. & Dependent on forward model fidelity; training complexity and coupled errors. \\
\hline
Probabilistic / disentangled models & Quantifies uncertainty and multimodality; exposes degeneracy; yields interpretable latent axes when successful. & Computationally expensive; latent axes not guaranteed physical without inductive bias. \\
\hline
\end{tabular}
\caption{Condensed comparison of interpretability strategies for physics-based inverse problems.}
\end{table}




\section{Research Question}

The challenges outlined in the previous sections—parameter degeneracy, data quality degradation, and lack of interpretability—converge to define a critical gap in the state of the art for nanoscale magnetic domain characterization. Current approaches struggle to simultaneously address the non-uniqueness of the inverse mapping, the prevalence of low-quality experimental data, and the opacity of data-driven models. This confluence of limitations motivates the central research question of this work:

\begin{center}
\textit{How can physics-informed deep learning frameworks be designed to simultaneously mitigate parameter degeneracy, enhance the quality of generated magnetic configurations, and ensure physical interpretability in the forward and inverse modeling of nanoscale magnetic domain states?}
\end{center}

This question is inherently multifaceted, requiring the integration of regularization strategies that respect the underlying Hamiltonian structure, generative methodologies capable of sampling physically plausible multimodal solutions, and interpretability mechanisms that expose the relationship between learned representations and observable physical quantities. Addressing this research question requires moving beyond purely data-driven approaches toward hybrid frameworks that embed physical constraints while maintaining computational tractability and generalization capacity across experimental and simulated magnetic imaging modalities.


\section{Objectives}

To address the research question formulated above, this work pursues three complementary objectives that tackle the core challenges of degeneracy, data quality, and interpretability in the modeling of nanoscale magnetic domain states:

\begin{enumerate}
    \item Develop a regularized deep learning framework based on Hamiltonian modeling refinement and transfer learning methodology to mitigate the effect of magnetic state degeneracy when performing a configuration estimation task.

    \item Develop a regularized deep learning framework based on generative models, employing diffusion strategies to enhance the quality of magnetic states generated from a related physical condition in a task of generating nanoscale magnetic domain states.

    \item Develop interpretability methodologies for deep learning models that take into account the underlying physical conditions that generated the analyzed magnetic domain states.
\end{enumerate}

\section{Methodology}\label{sec:Material_methods}


This section describes the methodological framework adopted to address the first research objective: mitigating the effects of magnetic state degeneracy when estimating nanoscale magnetic configurations from physical parameters. The methodology integrates (i) a Hamiltonian-based atomistic simulation pipeline for generating synthetic datasets, and (ii) a physically informed deep learning framework based on conditional variational autoencoders (cVAEs) enhanced with physics-based regularization and cycle-consistent modeling strategies.

\subsection{Hamiltonian-Based Dataset Generation}

The construction of the synthetic dataset is based on an atomistic Hamiltonian model that captures the main magnetic interactions governing nanoscale spin configurations. The total energy of the system is described by:

\begin{equation}
H = 
- \sum_{i=1}^{N} \sum_{j \in nn} J \left( \mathbf{S}_i \cdot \mathbf{S}_j \right)
+ \sum_{i=1}^{N} \sum_{j \in nn} \mathbf{D}_{ij} \cdot \left( \mathbf{S}_i \times \mathbf{S}_j \right)
- \sum_{i=1}^{N} \mathbf{h}_{\mathrm{ext}} \cdot \mathbf{S}_i
+ \sum_{i=1}^{N} K 
\left(
\alpha_{i1}^2 \alpha_{i2}^2 +
\alpha_{i2}^2 \alpha_{i3}^2 +
\alpha_{i3}^2 \alpha_{i1}^2
\right).
\label{eq:H_full}
\end{equation}

This Hamiltonian incorporates four fundamental contributions, each governing a distinct physical mechanism:

\paragraph{Exchange interaction.}
\[
H_{\mathrm{ex}} = - \sum_{i=1}^{N} \sum_{j \in nn} J \left( \mathbf{S}_i \cdot \mathbf{S}_j \right)
\]
The exchange constant $J$ controls the tendency of nearest-neighbor spins to align. Positive values favor ferromagnetic ordering, while negative values favor antiferromagnetic alignment. This term stabilizes smooth, low-energy magnetic domains.

\paragraph{Dzyaloshinskii--Moriya interaction (DMI).}
\[
H_{\mathrm{DMI}} = 
\sum_{i=1}^{N} \sum_{j \in nn} 
\mathbf{D}_{ij} \cdot \left( \mathbf{S}_i \times \mathbf{S}_j \right)
\]
The vector $\mathbf{D}_{ij}$ introduces chirality due to broken inversion symmetry, favoring non-collinear textures such as helices, Néel-type domain walls, and skyrmions.

\paragraph{Zeeman interaction.}
\[
H_{\mathrm{Z}} = 
- \sum_{i=1}^{N} \mathbf{h}_{\mathrm{ext}} \cdot \mathbf{S}_i
\]
The external field $\mathbf{h}_{\mathrm{ext}}$ couples to the spin orientations, tending to align the magnetization with the applied field direction.

\paragraph{Magnetocrystalline anisotropy.}
\[
H_{\mathrm{ani}} = 
\sum_{i=1}^{N} K 
\left(
\alpha_{i1}^2 \alpha_{i2}^2 +
\alpha_{i2}^2 \alpha_{i3}^2 +
\alpha_{i3}^2 \alpha_{i1}^2
\right)
\]
Here, $K$ denotes the anisotropy constant, and the components $\alpha_{ik}$ represent the direction cosines of the local spin orientation relative to the crystallographic axes. This term governs the preferred magnetization direction (easy or hard axes) and significantly influences domain morphology.

\vspace{0.3cm}
Together, these contributions enable exploration of a broad variety of magnetic textures, including vortices, stripes, labyrinth domains, helical states, and skyrmions.

\subsubsection*{Simulation Procedure}

Spin configurations are generated using the Metropolis Monte Carlo method, which ensures statistical sampling according to Boltzmann probabilities. Systematic sweeps of temperature, exchange constants, anisotropy parameters, DMI strength, and external magnetic fields define the parameter space. Each simulation produces:

\begin{itemize}
    \item a geometry file describing lattice coordinates of the nanodot,
    \item multiple state files encoding the spin orientations for each parameter combination.
\end{itemize}

\subsubsection*{Image Construction Pipeline}

The raw spin states are transformed into image-based representations through a structured preprocessing pipeline:

\begin{enumerate}
    \item \textbf{Geometry parsing:} lattice coordinates are loaded and masked to enforce the nanodot shape.
    \item \textbf{Spin mapping:} the $S_z$ component is assigned to each lattice site to reconstruct magnetization fields.
    \item \textbf{Image rendering:} a red--blue colormap encodes opposite out-of-plane spin orientations on a fixed $39 \times 39$ grid.
    \item \textbf{Normalization:} pixel intensities are scaled to either $[-1,1]$ or $[0,1]$.
    \item \textbf{Dataset storage:} each entry is saved as a NumPy array containing the image and the corresponding Hamiltonian parameter vector.
\end{enumerate}

The resulting dataset contains tens of thousands of labeled images spanning a wide range of physical regimes, from fully ordered low-temperature states to disordered high-temperature configurations. This dataset constitutes the controlled foundation for training deep learning architectures in both forward and inverse modeling tasks.


\subsection{Physically Informed Conditional Variational Autoencoder}

A physically informed conditional variational autoencoder (cVAE) framework is adopted to address the degeneracy inherent to the forward problem, where multiple magnetic textures can arise from similar Hamiltonian parameters. The model learns a conditional distribution over domain configurations given a physical parameter vector $\mathbf{c}$, enforcing physical coherence through the latent structure and the learning objective.

The cVAE consists of an encoder $q_{\phi}(\mathbf{z}|\mathbf{x},\mathbf{c})$ producing a Gaussian latent embedding with mean $\mu_{\phi}(\mathbf{x},\mathbf{c})$ and variance $\sigma^2_{\phi}(\mathbf{x},\mathbf{c})$, and a decoder $p_{\theta}(\mathbf{x}|\mathbf{z},\mathbf{c})$ reconstructing the magnetic configuration. Conditioning is performed by mapping the Hamiltonian parameters into a physically informed latent prior $p_{\theta}(\mathbf{z}|\mathbf{c})$ through a parametric network, following formulations used in physics-guided variational inference frameworks \cite{lee2021hamiltonianVAE, hermans2024cvai}.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{Figures/pcVae.png}
    \caption{Conditional VAE architecture with physics-guided parameter conditioning.}
    \label{fig:pcvae_architecture}
\end{figure}

The total training objective is defined as:
\begin{equation}
\mathcal{L}_{\mathrm{total}} 
= 
\mathcal{L}_{\mathrm{rec}}
+ \beta \mathcal{L}_{\mathrm{KL}}
+ \gamma \mathcal{L}_{\mathrm{phys}},
\end{equation}
where each component targets a specific aspect of physical or statistical consistency.

\subsubsection{Conditional KL Divergence}

The KL divergence regularizes the posterior distribution by enforcing proximity to a parameter-dependent latent prior:
\begin{equation}
\mathcal{L}_{\mathrm{KL}}
=
D_{\mathrm{KL}}
\left(
q_{\phi}(\mathbf{z}|\mathbf{x},\mathbf{c})
\;\|\;
p_{\theta}(\mathbf{z}|\mathbf{c})
\right).
\end{equation}

Unlike classical VAEs—which impose a fixed standard normal prior $p(\mathbf{z}) = \mathcal{N}(0, I)$—the conditional prior $p_{\theta}(\mathbf{z}|\mathbf{c})$ introduces a physics-shaped structure into the latent space. This prior is implemented as:
\begin{equation}
p_{\theta}(\mathbf{z}|\mathbf{c}) 
=
\mathcal{N}
\big(
\mu_{\theta}(\mathbf{c}),\;
\Sigma_{\theta}(\mathbf{c})
\big),
\end{equation}
where $\mu_{\theta}$ and $\Sigma_{\theta}$ are neural networks that learn a latent distribution consistent with the Hamiltonian's parameter space.

This formulation prevents the model from collapsing physically distinct regimes into overlapping latent regions, reducing degeneracy by enforcing that different parameter vectors induce distinct latent priors. Consequently, the latent geometry reflects the topology of the physical system.

\subsubsection{Physics-Guided Energy Penalties}

The physics-based term $\mathcal{L}_{\mathrm{phys}}$ constrains the decoder to produce configurations consistent with low-energy magnetic states. Given the Hamiltonian:
\begin{equation}
H(\mathbf{x};\mathbf{c}) 
=
H_{\mathrm{ex}} + H_{\mathrm{DMI}} + H_{\mathrm{Z}} + H_{\mathrm{ani}},
\end{equation}
each component can be used to penalize deviations from expected energetics:
\begin{align}
H_{\mathrm{ex}} &= 
- J \sum_{i} \sum_{j \in nn} 
(\mathbf{S}_i \cdot \mathbf{S}_j), \\
H_{\mathrm{DMI}} &= 
\sum_{i} \sum_{j \in nn} 
\mathbf{D}_{ij} \cdot (\mathbf{S}_i \times \mathbf{S}_j), \\
H_{\mathrm{Z}} &= 
- \sum_{i} \mathbf{h}_{\mathrm{ext}} \cdot \mathbf{S}_i, \\
H_{\mathrm{ani}} &= 
\sum_{i} 
K (\alpha_{i1}^2 \alpha_{i2}^2 + \alpha_{i2}^2 \alpha_{i3}^2 + \alpha_{i3}^2 \alpha_{i1}^2).
\end{align}

A general physics penalty can be expressed as:
\begin{equation}
\mathcal{L}_{\mathrm{phys}}
=
\lambda_{\mathrm{E}}
\left(
H(\hat{\mathbf{x}};\mathbf{c})
-
H_{\mathrm{ref}}(\mathbf{c})
\right)^2
+
\sum_{k}
\lambda_k \, \Psi_k(\hat{\mathbf{x}},\mathbf{c}),
\end{equation}
where:

- $\hat{\mathbf{x}}$ is the reconstructed or generated configuration,
- $H_{\mathrm{ref}}(\mathbf{c})$ is a reference or minimum-energy estimate for the given parameters,
- $\Psi_k$ are optional additional constraints such as:
  - chiral consistency for DMI systems,
  - topological charge regularization for skyrmion textures,
  - divergence penalties for enforcing $\|\nabla \cdot \mathbf{m}\| \approx 0$ where applicable.

Examples include:
\begin{align}
\Psi_{\mathrm{chirality}}
&=
\left\|
(\mathbf{S}_i \times \mathbf{S}_j)\cdot \hat{\mathbf{D}}_{ij}
-
\chi_{\mathrm{target}}(\mathbf{c})
\right\|^2,
\\[4pt]
\Psi_{\mathrm{topo}}
&=
\left(
Q(\hat{\mathbf{x}}) - Q_{\mathrm{expected}}(\mathbf{c})
\right)^2,
\end{align}
where $Q$ denotes the skyrmion number.

These penalties restrict the learned manifold to physically meaningful magnetic textures, mitigating degeneracy by discarding energetically unstable or symmetry-inconsistent solutions that would otherwise satisfy the reconstruction objective.


Once the model has been trained, inference proceeds by conditioning the generative process on the physical parameter vector associated with the target magnetic regime. The parameter vector is first processed through a multilayer perceptron (MLP) that constructs the conditional latent prior distribution. A latent sample is then drawn by combining the prior mean and variance with a stochastic perturbation sampled from a standard normal distribution, producing the latent representation used during generation. This latent code is subsequently merged with a secondary conditioning network that injects the physical parameters into the generative pathway, ensuring consistency with the Hamiltonian-defined regime. Finally, the decoder maps the conditioned latent vector into a magnetic domain configuration, yielding an image that is statistically coherent, physically plausible, and aligned with the specified material and interaction parameters. This prediction-stage pipeline enables the model to infer nanoscale magnetic states directly from physical conditions while preserving physical regularity learned during training.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{Figures/inference.png}
    \caption{Prediction Phase}
    \label{fig:pcvae_architecture}
\end{figure}

\subsection{Cycle-Consistent Forward--Inverse Framework}

To reduce parameter degeneracy and reinforce physical identifiability, a cycle-consistent framework is incorporated on top of the physically informed conditional generative model. The core idea is to exploit the structure learned by the forward generator to regularize the inverse mapping, enforcing that parameter estimates remain consistent with the physical conditions that originally produced each magnetic configuration.

\subsubsection{Forward Model}
The forward model corresponds to the conditional generator $G_{\theta}$ already trained through the physically informed cVAE framework. Given a Hamiltonian parameter vector $\mathbf{p}$, the generator produces a corresponding magnetic configuration:
\begin{equation}
\hat{\mathbf{x}} = G_{\theta}(\mathbf{p}, \mathbf{z}),
\end{equation}
where $\mathbf{z}$ is sampled from the conditional latent prior. The generator has already been optimized using:
\begin{equation}
\mathcal{L}_{\mathrm{forward}} 
= \mathcal{L}_{\mathrm{rec}}
+ \beta \mathcal{L}_{\mathrm{KL}}
+ \gamma \mathcal{L}_{\mathrm{phys}}.
\end{equation}

\subsubsection{Inverse Model}
A complementary inverse estimator $F_{\phi}$ is introduced to recover the Hamiltonian parameters from generated or real images:
\begin{equation}
\hat{\mathbf{p}} = F_{\phi}(\mathbf{x}).
\end{equation}
The inverse model is trained with a supervised regression loss over the synthetic dataset:
\begin{equation}
\mathcal{L}_{\mathrm{inv}} = 
\left\| F_{\phi}(\mathbf{x}) - \mathbf{p} \right\|_{2}^{2}.
\end{equation}

\subsubsection{Cycle Consistency Constraint}
The forward and inverse models are coupled through cycle consistency. The cycle ensures that parameters reconstructed after a forward--inverse pass remain close to the original ones:
\begin{equation}
\mathbf{p} \xrightarrow{G_{\theta}} \hat{\mathbf{x}} 
\xrightarrow{F_{\phi}} \hat{\mathbf{p}}.
\end{equation}

The cycle-consistency penalty is defined as:
\begin{equation}
\mathcal{L}_{\mathrm{cycle}}
= \left\| F_{\phi}\left( G_{\theta}(\mathbf{p}, \mathbf{z}) \right) - \mathbf{p} \right\|_{2}^{2}.
\end{equation}

This term forces the inverse model to be consistent with the learned physics of the generator and simultaneously restricts the generator to produce parameter-identifiable magnetic states.

\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{Figures/Cycle.png}
    \caption{Cycle Consistency Constraint}
    \label{fig:pcvae_architecture}
\end{figure}

\subsubsection{Closed-Loop Joint Objective}

During joint training, the generator and the inverse estimator are optimized under a combined loss:
\begin{equation}
\mathcal{L}_{\mathrm{joint}}
= \mathcal{L}_{\mathrm{forward}}
+ \lambda_{\mathrm{inv}} \mathcal{L}_{\mathrm{inv}}
+ \lambda_{\mathrm{cycle}} \mathcal{L}_{\mathrm{cycle}}.
\end{equation}

\paragraph{Interpretation of Each Term.}
\begin{itemize}
    \item $\mathcal{L}_{\mathrm{forward}}$ enforces physical plausibility and accurate magnetic configuration synthesis.
    \item $\mathcal{L}_{\mathrm{inv}}$ trains the inverse model to regress Hamiltonian parameters from images.
    \item $\mathcal{L}_{\mathrm{cycle}}$ enforces round-trip (parameter $\rightarrow$ image $\rightarrow$ parameter) consistency.
\end{itemize}

\subsubsection{Training Strategy}
Training proceeds in two stages:

\paragraph{Stage 1: Pretraining the Forward Model.}
Only the cVAE-based generator is trained using the physically informed forward loss. This stabilizes the latent space and ensures physically coherent sampling.

\paragraph{Stage 2: Cycle-Consistent Joint Training.}
The forward generator $G_{\theta}$ and the inverse estimator $F_{\phi}$ are co-optimized. The update rules are:
\begin{align}
\theta &\leftarrow \theta - \eta_{\theta} 
\nabla_{\theta} \left(
\mathcal{L}_{\mathrm{forward}}
+ \lambda_{\mathrm{cycle}} \mathcal{L}_{\mathrm{cycle}}
\right), \\
\phi &\leftarrow \phi - \eta_{\phi}
\nabla_{\phi} \left(
\lambda_{\mathrm{inv}} \mathcal{L}_{\mathrm{inv}} 
+ \lambda_{\mathrm{cycle}} \mathcal{L}_{\mathrm{cycle}}
\right).
\end{align}

Note that the forward loss does not directly update the inverse model, whereas cycle loss updates both networks simultaneously, enforcing bidirectional consistency.

\subsubsection{Hyperparameters of the Full Framework}
The complete system introduces several interpretable hyperparameters:
\begin{itemize}
    \item $\beta$: KL divergence weight (latent regularization).
    \item $\gamma$: physical-energy penalty weight.
    \item $\lambda_{\mathrm{inv}}$: strength of the inverse-regression term.
    \item $\lambda_{\mathrm{cycle}}$: strength of the cycle-consistency constraint.
    \item $\eta_{\theta}$, $\eta_{\phi}$: learning rates of forward and inverse models.
\end{itemize}

These hyperparameters govern the trade-offs between reconstruction fidelity, latent regularization, physical consistency, inverse accuracy, and closed-loop identifiability. Proper selection enables the framework to perform robust forward synthesis and stable parameter inference despite degeneracy in the image space.



\subsection{Bayesian Hyperparameter Optimization}

The full training pipeline introduces several interacting hyperparameters  
that govern regularization strength, latent dimensionality, cycle constraints, and learning dynamics.  
Because the loss landscape is highly non-convex and the model exhibits multimodal behaviors arising from magnetic degeneracy, a principled hyperparameter search strategy is required.  
To this end, Bayesian hyperparameter optimization is employed to systematically identify the optimal configuration
\begin{equation}
\boldsymbol{\lambda}^{\star} = 
\arg\min_{\boldsymbol{\lambda} \in \Lambda} 
\mathcal{J}(\boldsymbol{\lambda}),
\end{equation}
where $\mathcal{J}$ denotes the validation objective (e.g., a weighted combination of reconstruction error, inverse-parameter accuracy, and physical energy consistency).

\subsubsection{Surrogate Modeling of the Objective}
Instead of directly evaluating $\mathcal{J}(\boldsymbol{\lambda})$ for every configuration, Bayesian optimization constructs a probabilistic surrogate model $M$—typically a Gaussian Process (GP)—to approximate the unknown objective function.  
Given a set of observed evaluations 
$\mathcal{D}_{t} = \{ (\boldsymbol{\lambda}_{i}, y_{i}) \}_{i=1}^{t}$,
the GP surrogate models the objective as:
\begin{equation}
\mathcal{J}(\boldsymbol{\lambda}) \sim 
\mathcal{GP}\big( m(\boldsymbol{\lambda}),\, 
k(\boldsymbol{\lambda}, \boldsymbol{\lambda}') \big),
\end{equation}
where $m$ is the mean function (often taken as zero) and $k$ is a covariance kernel (e.g., Matérn or squared exponential).  
The GP posterior provides both a predicted mean $\mu_{t}(\boldsymbol{\lambda})$ and uncertainty $\sigma_{t}(\boldsymbol{\lambda})$, enabling guided exploration.

\subsubsection{Acquisition Function}
Bayesian optimization selects the next hyperparameter candidate by maximizing an acquisition function $\alpha(\boldsymbol{\lambda}; \mathcal{D}_{t})$ that balances exploration and exploitation.  
A commonly used acquisition function is *Expected Improvement* (EI), defined as:
\begin{equation}
\alpha_{\mathrm{EI}}(\boldsymbol{\lambda}) 
= \mathbb{E}\big[ \max(0,\, \mathcal{J}_{\mathrm{best}} - \mathcal{J}(\boldsymbol{\lambda}) ) \big],
\end{equation}
which has the closed-form expression:
\begin{equation}
\alpha_{\mathrm{EI}}(\boldsymbol{\lambda}) 
= (\mathcal{J}_{\mathrm{best}} - \mu_{t}(\boldsymbol{\lambda})) 
\Phi(Z) + \sigma_{t}(\boldsymbol{\lambda}) \phi(Z),
\qquad
Z = \frac{\mathcal{J}_{\mathrm{best}} - \mu_{t}(\boldsymbol{\lambda})}{\sigma_{t}(\boldsymbol{\lambda})},
\end{equation}
where $\Phi$ and $\phi$ denote the standard normal CDF and PDF respectively.

This acquisition function assigns high values to regions where:
\begin{itemize}
    \item the GP predicts low objective values (\textit{exploitation}), or
    \item the model uncertainty $\sigma_{t}$ is high (\textit{exploration}).
\end{itemize}

\subsubsection{Sequential Optimization Loop}
At each iteration, the Bayesian optimization loop performs:
\begin{enumerate}
    \item Update the GP surrogate using all observed evaluations.
    \item Select the next hyperparameter configuration:
    \begin{equation}
    \boldsymbol{\lambda}_{t+1}
    = \arg\max_{\boldsymbol{\lambda} \in \Lambda}
    \alpha(\boldsymbol{\lambda}; \mathcal{D}_{t}).
    \end{equation}
    \item Train the forward--inverse system using $\boldsymbol{\lambda}_{t+1}$ and compute the validation score:
    \begin{equation}
    y_{t+1} = \mathcal{J}(\boldsymbol{\lambda}_{t+1}).
    \end{equation}
    \item Expand the dataset:
    \begin{equation}
    \mathcal{D}_{t+1} = \mathcal{D}_{t} \cup \{ (\boldsymbol{\lambda}_{t+1}, y_{t+1}) \}.
    \end{equation}
\end{enumerate}

The process continues until convergence or until a fixed evaluation budget is exhausted.  
The final estimate of the optimal hyperparameters is:
\begin{equation}
\boldsymbol{\lambda}^{\star}
= \arg\min_{(\boldsymbol{\lambda}_{i},\, y_{i}) \in \mathcal{D}_{T}}
y_{i}.
\end{equation}


\begin{figure}[H]
    \centering
    \includegraphics[width=0.90\textwidth]{Figures/Bayesian2.png}
    \caption{Bayesian Optimization}
    \label{fig:pcvae_architecture}
\end{figure}

\subsubsection{Advantages for the Physical Modeling Context}
Bayesian optimization is particularly suitable for this framework because:
\begin{itemize}
    \item training is computationally expensive, making random or grid search inefficient;
    \item the objective is noisy due to stochastic optimization and Monte Carlo sampling;
    \item degeneracy creates multimodal structures in the hyperparameter landscape;
    \item Bayesian methods exploit uncertainties to escape local minima and navigate parameter regimes where physical constraints dominate.
\end{itemize}

Thus, Bayesian hyperparameter optimization provides a mathematically grounded mechanism for discovering optimal regularization weights, latent space dimensionality, and cycle-consistency strengths that maximize physical fidelity and inverse-model identifiability.

\subsection{Mitigating Low-Quality Data in the Generative Pipeline}

Two complementary strategies are considered to improve the quality and realism of generated magnetic-domain images when the synthetic forward model or training images are insufficiently realistic: (A) adversarial enhancement of generated images using a discriminator trained on high-quality references, and (B) domain adaptation (sim2real) when experimental images are available. Both strategies can be combined with the physically informed cVAE generator described previously.

\subsubsection{A. Adversarial Enhancement (Conditional GAN on top of the cVAE generator)}

\paragraph{Concept.}
A conditional adversarial discriminator $D_{\psi}$ is introduced to distinguish high-quality magnetic images $x^{H}$ (real high-quality references) from generated images $\hat{x}$ produced by the cVAE generator $G_{\theta}(z,c)$. Conditioning on the Hamiltonian parameters $c$ or other metadata is recommended so that the discriminator judges both visual realism and consistency with the conditioned regime. The generator is therefore trained to minimize an adversarial loss that complements the reconstruction, KL and physics penalties.

\paragraph{Architecture (schematic).}
\[
z \sim p_{\theta}(z|c) \quad\rightarrow\quad \hat{x} = G_{\theta}(z,c)
\]
\[
D_{\psi}(x, c) \in [0,1] \quad\text{(probability that } x \text{ is real high-quality)}
\]

\paragraph{Losses.}
A typical choice is a least-squares GAN (LSGAN) or hinge loss (both are more stable than vanilla GAN). The discriminator loss (LSGAN form) is:
\begin{equation}
\mathcal{L}_{D} = \frac{1}{2}\mathbb{E}_{x^{H},c}\left[ \big(D_{\psi}(x^{H},c) - 1\big)^2 \right]
+ \frac{1}{2}\mathbb{E}_{z,c}\left[ \big(D_{\psi}(\hat{x},c)\big)^2 \right].
\label{eq:lsgan_D}
\end{equation}

The adversarial component of the generator loss is:
\begin{equation}
\mathcal{L}_{\mathrm{adv}} = \frac{1}{2}\mathbb{E}_{z,c}\left[ \big(D_{\psi}(\hat{x},c) - 1\big)^2 \right].
\label{eq:lsgan_G}
\end{equation}

\begin{figure}[H]
    \centering
    \includegraphics[width=0.60\textwidth]{Figures/GANS.png}
    \caption{Conditional GAN}
    \label{fig:pcvae_architecture}
\end{figure}
The generator total loss augments the previously defined cVAE loss:
\begin{equation}
\mathcal{L}_{G} = \mathcal{L}_{\mathrm{rec}} + \beta \mathcal{L}_{\mathrm{KL}} + \gamma \mathcal{L}_{\mathrm{phys}}
+ \lambda_{\mathrm{adv}}\mathcal{L}_{\mathrm{adv}} + \lambda_{\mathrm{perc}}\mathcal{L}_{\mathrm{perc}} + \lambda_{\mathrm{tv}}\mathcal{L}_{\mathrm{tv}},
\label{eq:gen_total_with_adv}
\end{equation}
where:
\begin{itemize}
  \item $\mathcal{L}_{\mathrm{perc}} = \mathbb{E}\big[\| \phi(\hat{x}) - \phi(x^{H})\|_2^2\big]$ is a perceptual loss computed on features $\phi(\cdot)$ of a pretrained network (e.g., VGG-like features or a domain-specific feature extractor) to preserve fine structural details (domain walls, cores).
  \item $\mathcal{L}_{\mathrm{tv}}$ is a total-variation regularizer to suppress high-frequency artifacts.
  \item $\lambda_{\mathrm{adv}}, \lambda_{\mathrm{perc}}, \lambda_{\mathrm{tv}}$ are scalar weights.
\end{itemize}











\section{Results} \label{sec:Exp}






\section{Conclusions} \label{sec:Conclusions}



\section{Future Work}\label{sec:future_work}


\bibliographystyle{unsrt}
\small{
\bibliography{references}}

\end{document}


