# Models - Entrenamiento de Redes Neuronales

## ğŸ“‹ DescripciÃ³n

Esta carpeta contiene los notebooks de entrenamiento de diferentes arquitecturas de redes neuronales convolucionales (CNNs) para la predicciÃ³n de parÃ¡metros hamilton

ianos a partir de configuraciones de spin magnÃ©ticas.

## ğŸ—‚ï¸ Estructura

```
Models/
â”œâ”€â”€ DatabaseJex2T/              # Modelos para predecir Jex2 y Temperatura
â”‚   â”œâ”€â”€ DenseNetFinal.ipynb      âœ… MEJOR MODELO (RÂ²=0.9753)
â”‚   â”œâ”€â”€ EfficienNetFinal.ipynb
â”‚   â””â”€â”€ ResNetFinal.ipynb
â”‚
â””â”€â”€ DatabaseKDMT/               # Modelos para predecir KDM y Temperatura
    â”œâ”€â”€ DenseNet121KDM.ipynb
    â”œâ”€â”€ EfficientNetB2KDM.ipynb
    â”œâ”€â”€ EfficientNetKDM.ipynb
    â”œâ”€â”€ InceptionNetKDM.ipynb
    â””â”€â”€ ResNetKDM.ipynb
```

## ğŸ† ComparaciÃ³n de Arquitecturas

### DatabaseJex2T (PredicciÃ³n de Jex2)

| Modelo | RÂ² Score | MAPE | SMAPE | ParÃ¡metros | Tiempo/Ã‰poca |
|--------|----------|------|-------|------------|--------------|
| **DenseNet121** âœ… | **0.9753** | **18.64%** | **15.49%** | ~7M | ~3 min |
| ResNet50 | ~0.94 | ~22% | ~18% | ~23M | ~2 min |
| EfficientNetB2 | ~0.95 | ~20% | ~17% | ~7.8M | ~4 min |

**Ganador:** DenseNet121 por mejor RÂ² y balance eficiencia/performance

### DatabaseKDMT (PredicciÃ³n de KDM)

| Modelo | Estado | Observaciones |
|--------|--------|---------------|
| DenseNet121KDM | En progreso | Arquitectura prometedora |
| EfficientNetB2KDM | En progreso | Balance eficiencia/performance |
| EfficientNetKDM | En progreso | VersiÃ³n B7 mÃ¡s pesada |
| InceptionNetKDM | En progreso | Multi-scale features |
| ResNetKDM | En progreso | Baseline con residuals |

## ğŸ”„ Pipeline de Entrenamiento

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 1. CARGA DE DATOS                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ â€¢ Descarga dataset desde Kaggle/Drive            â”‚     â”‚
â”‚  â”‚ â€¢ Carga archivo .npz                             â”‚     â”‚
â”‚  â”‚ â€¢ ExtracciÃ³n: X (imÃ¡genes), y (parÃ¡metros)      â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              2. PREPROCESAMIENTO                           â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ â€¢ Resize: 42Ã—42 â†’ 224Ã—224 (TF resize)           â”‚     â”‚
â”‚  â”‚ â€¢ ConversiÃ³n: Grayscale â†’ RGB (3 canales)       â”‚     â”‚
â”‚  â”‚ â€¢ NormalizaciÃ³n imÃ¡genes (global min/max)       â”‚     â”‚
â”‚  â”‚ â€¢ MinMaxScaler en targets (y)                   â”‚     â”‚
â”‚  â”‚ â€¢ Train/Val split: 90/10 (stratified)          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚              3. CONSTRUCCIÃ“N DEL MODELO                    â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Base Model (Pre-trained on ImageNet)            â”‚     â”‚
â”‚  â”‚  â”œâ”€ DenseNet121 / ResNet50 / EfficientNet      â”‚     â”‚
â”‚  â”‚  â”œâ”€ Freeze initial layers                       â”‚     â”‚
â”‚  â”‚  â””â”€ Trainable: Ãºltimas N capas                  â”‚     â”‚
â”‚  â”‚                                                  â”‚     â”‚
â”‚  â”‚ Custom Head:                                     â”‚     â”‚
â”‚  â”‚  â”œâ”€ GlobalAveragePooling2D()                    â”‚     â”‚
â”‚  â”‚  â”œâ”€ Dense(256, relu, dropout=0.3)              â”‚     â”‚
â”‚  â”‚  â”œâ”€ Dense(128, relu, dropout=0.2)              â”‚     â”‚
â”‚  â”‚  â””â”€ Dense(1, linear) â† RegresiÃ³n               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  4. ENTRENAMIENTO                          â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ Optimizer: Adam (lr=1e-4, decay=1e-6)           â”‚     â”‚
â”‚  â”‚ Loss: Mean Squared Error (MSE)                   â”‚     â”‚
â”‚  â”‚ Metrics: MAE, MAPE                              â”‚     â”‚
â”‚  â”‚                                                  â”‚     â”‚
â”‚  â”‚ Callbacks:                                       â”‚     â”‚
â”‚  â”‚  â€¢ ModelCheckpoint (save_best_only=True)        â”‚     â”‚
â”‚  â”‚  â€¢ EarlyStopping (patience=10, monitor='val_loss')â”‚   â”‚
â”‚  â”‚  â€¢ ReduceLROnPlateau (factor=0.5, patience=5)   â”‚     â”‚
â”‚  â”‚                                                  â”‚     â”‚
â”‚  â”‚ Batch size: 32                                   â”‚     â”‚
â”‚  â”‚ Epochs: 50-100 (early stopping)                 â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 5. EVALUACIÃ“N                              â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ MÃ©tricas en Validation Set:                     â”‚     â”‚
â”‚  â”‚  â€¢ RÂ² Score                                      â”‚     â”‚
â”‚  â”‚  â€¢ MAPE (masked, epsilon=1e-8)                  â”‚     â”‚
â”‚  â”‚  â€¢ SMAPE                                         â”‚     â”‚
â”‚  â”‚                                                  â”‚     â”‚
â”‚  â”‚ Visualizaciones:                                 â”‚     â”‚
â”‚  â”‚  â€¢ Scatter: y_real vs y_pred                    â”‚     â”‚
â”‚  â”‚  â€¢ Learning curves (loss vs epochs)             â”‚     â”‚
â”‚  â”‚  â€¢ Residual plots                               â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚
                 â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                 6. GUARDADO                                â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”     â”‚
â”‚  â”‚ â€¢ Modelo completo: .h5 format                   â”‚     â”‚
â”‚  â”‚ â€¢ Pesos: checkpoint.weights.h5                   â”‚     â”‚
â”‚  â”‚ â€¢ Historial: history.json                       â”‚     â”‚
â”‚  â”‚ â€¢ Scaler: scaler.pkl (para inferencia)          â”‚     â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

## ğŸ—ï¸ Arquitecturas Implementadas

### 1. DenseNet121 â­ MEJOR MODELO

**Arquitectura:**
```
Input (224, 224, 3)
    â†“
DenseNet121 Base (ImageNet weights)
â”œâ”€ Conv2D (7Ã—7, stride=2)
â”œâ”€ MaxPooling (3Ã—3, stride=2)
â”œâ”€ Dense Block 1 (6 layers)
â”œâ”€ Transition Layer 1
â”œâ”€ Dense Block 2 (12 layers)
â”œâ”€ Transition Layer 2
â”œâ”€ Dense Block 3 (24 layers)
â”œâ”€ Transition Layer 3
â””â”€ Dense Block 4 (16 layers)
    â†“
GlobalAveragePooling2D
    â†“
Dense(256, relu) + Dropout(0.3)
    â†“
Dense(128, relu) + Dropout(0.2)
    â†“
Dense(1, linear) â† Output
```

**Ventajas:**
- âœ… Conexiones densas â†’ mejor flujo de gradientes
- âœ… Feature reuse â†’ menos parÃ¡metros (~7M vs ~23M ResNet)
- âœ… Mejor generalizaciÃ³n en datasets pequeÃ±os/medianos
- âœ… RÂ² = 0.9753 en Jex2

**HiperparÃ¡metros:**
```python
base_model = DenseNet121(include_top=False,
                        weights='imagenet',
                        input_shape=(224, 224, 3))

# Freeze primeras capas
for layer in base_model.layers[:-50]:
    layer.trainable = False

optimizer = Adam(learning_rate=1e-4, decay=1e-6)
loss = 'mse'
```

### 2. ResNet50

**Arquitectura:**
```
Input (224, 224, 3)
    â†“
ResNet50 Base (ImageNet weights)
â”œâ”€ Conv2D (7Ã—7, stride=2)
â”œâ”€ MaxPooling (3Ã—3, stride=2)
â”œâ”€ Conv Block 1 (3 layers) + Skip
â”œâ”€ Conv Block 2 (4 layers) + Skip
â”œâ”€ Conv Block 3 (6 layers) + Skip
â”œâ”€ Conv Block 4 (3 layers) + Skip
â””â”€ Conv Block 5 (3 layers) + Skip
    â†“
GlobalAveragePooling2D
    â†“
Dense(512, relu) + Dropout(0.4)
    â†“
Dense(256, relu) + Dropout(0.3)
    â†“
Dense(1, linear) â† Output
```

**Ventajas:**
- âœ… Residual connections â†’ entrenamientos muy profundos
- âœ… Arquitectura muy probada y estable
- âœ… Baseline confiable

**Desventajas:**
- âŒ MÃ¡s parÃ¡metros (~23M)
- âŒ Ligeramente inferior a DenseNet en este problema

### 3. EfficientNetB2/B7

**Arquitectura:**
```
Input (224, 224, 3)
    â†“
EfficientNet Base (Compound Scaling)
â”œâ”€ Stem: Conv2D (3Ã—3)
â”œâ”€ MBConv Blocks (depth, width, resolution scaling)
â”‚   â”œâ”€ Depthwise Conv
â”‚   â”œâ”€ Squeeze-Excitation
â”‚   â””â”€ Skip connection
â””â”€ Head: Conv2D (1Ã—1)
    â†“
GlobalAveragePooling2D
    â†“
Dense(256, relu) + Dropout(0.3)
    â†“
Dense(1, linear) â† Output
```

**Ventajas:**
- âœ… Compound scaling balanceado
- âœ… Eficiente en parÃ¡metros y FLOPs
- âœ… State-of-the-art en ImageNet

**Desventajas:**
- âŒ B7 es muy pesado para este problema
- âŒ B2 competitivo pero no supera DenseNet

### 4. InceptionNetV3

**Arquitectura:**
```
Input (224, 224, 3)
    â†“
InceptionNet Base
â”œâ”€ Conv2D inicial
â”œâ”€ Inception Module A (multi-scale 1Ã—1, 3Ã—3, 5Ã—5)
â”œâ”€ Reduction A
â”œâ”€ Inception Module B
â”œâ”€ Reduction B
â””â”€ Inception Module C
    â†“
GlobalAveragePooling2D
    â†“
Dense(512, relu) + Dropout(0.4)
    â†“
Dense(256, relu) + Dropout(0.3)
    â†“
Dense(1, linear) â† Output
```

**Ventajas:**
- âœ… Multi-scale feature extraction
- âœ… Captura patrones a diferentes escalas

**AplicaciÃ³n:** Ãštil si dominios magnÃ©ticos tienen estructuras multi-escala

## ğŸ“Š MÃ©tricas de EvaluaciÃ³n

### RÂ² Score (Coeficiente de DeterminaciÃ³n)
```python
from sklearn.metrics import r2_score

r2 = r2_score(y_real, y_pred)
# InterpretaciÃ³n:
# 1.0 = PredicciÃ³n perfecta
# 0.0 = Modelo no mejor que predecir la media
# <0.0 = Modelo peor que predecir la media
```

### MAPE (Mean Absolute Percentage Error)
```python
def masked_mape(y_true, y_pred, epsilon=1e-8):
    """MAPE con mÃ¡scara para evitar divisiÃ³n por cero"""
    mask = np.abs(y_true) > epsilon
    return np.mean(np.abs((y_true[mask] - y_pred[mask]) / y_true[mask])) * 100
```

### SMAPE (Symmetric Mean Absolute Percentage Error)
```python
def smape(y_true, y_pred, epsilon=1e-8):
    """SMAPE simÃ©trico, mÃ¡s robusto que MAPE"""
    denominator = np.abs(y_true) + np.abs(y_pred) + epsilon
    return np.mean(2 * np.abs(y_true - y_pred) / denominator) * 100
```

## ğŸ›ï¸ HiperparÃ¡metros Clave

### Transfer Learning Strategy

```python
# Estrategia 1: Freeze + Fine-tuning
base_model.trainable = False  # Entrenar solo head
# ... entrenar 10 epochs ...
base_model.trainable = True   # Unfreeze
for layer in base_model.layers[:-50]:
    layer.trainable = False   # Freeze solo primeras capas
# ... entrenar 40 epochs mÃ¡s ...

# Estrategia 2: Freeze parcial desde inicio
for layer in base_model.layers[:-50]:
    layer.trainable = False
# ... entrenar 50 epochs ...
```

### Learning Rate Schedule

```python
# OpciÃ³n 1: ReduceLROnPlateau
ReduceLROnPlateau(monitor='val_loss',
                  factor=0.5,
                  patience=5,
                  min_lr=1e-7)

# OpciÃ³n 2: Cosine Decay
tf.keras.optimizers.schedules.CosineDecay(
    initial_learning_rate=1e-4,
    decay_steps=1000)
```

### RegularizaciÃ³n

```python
# Dropout en head
Dense(256, activation='relu')
Dropout(0.3)  # 30% dropout

# L2 Regularization
Dense(256, activation='relu',
      kernel_regularizer=l2(0.01))

# Data Augmentation (si aplica)
tf.keras.layers.RandomFlip("horizontal")
tf.keras.layers.RandomRotation(0.2)
```

## ğŸš€ CÃ³mo Ejecutar un Entrenamiento

### Paso 1: Preparar Entorno

```python
# En Google Colab
from google.colab import drive
drive.mount('/content/drive')

# Instalar dependencias
!pip install -q umap-learn
```

### Paso 2: Cargar Dataset

```python
import numpy as np

file_ = '/content/drive/MyDrive/DoctoradoPaper1/DataSets/spinesv0.npz'
data = np.load(file_)

X = data['X'][:, :42, :, :]  # ImÃ¡genes
y_jex2 = data['y'][:, 0].reshape(-1, 1)  # Target Jex2
y_T = data['y'][:, 1].reshape(-1, 1)     # Target Temperatura
```

### Paso 3: Preprocesar

```python
import tensorflow as tf
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split

# Resize a 224Ã—224
def preprocess_image(image):
    image = tf.image.resize(image, (224, 224))
    return image

processed_images = np.array([preprocess_image(img) for img in X])

# Normalizar targets
scaler = MinMaxScaler()
y_scaled = scaler.fit_transform(y_jex2)

# Split
X_train, X_val, y_train, y_val = train_test_split(
    processed_images, y_scaled, test_size=0.1, random_state=42
)

# Crear tf.data.Dataset
train_dataset = tf.data.Dataset.from_tensor_slices((X_train, y_train))
train_dataset = train_dataset.shuffle(1000).batch(32).prefetch(tf.data.AUTOTUNE)
```

### Paso 4: Construir Modelo

```python
from tensorflow.keras.applications import DenseNet121
from tensorflow.keras.layers import GlobalAveragePooling2D, Dense, Dropout
from tensorflow.keras.models import Model

# Base model
base = DenseNet121(include_top=False, weights='imagenet',
                   input_shape=(224, 224, 3))

# Freeze
for layer in base.layers[:-50]:
    layer.trainable = False

# Head
x = base.output
x = GlobalAveragePooling2D()(x)
x = Dense(256, activation='relu')(x)
x = Dropout(0.3)(x)
x = Dense(128, activation='relu')(x)
x = Dropout(0.2)(x)
output = Dense(1, activation='linear')(x)

model = Model(inputs=base.input, outputs=output)
```

### Paso 5: Compilar y Entrenar

```python
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping

model.compile(
    optimizer=Adam(1e-4),
    loss='mse',
    metrics=['mae']
)

callbacks = [
    ModelCheckpoint('best_model.h5', save_best_only=True),
    EarlyStopping(patience=10, restore_best_weights=True),
    ReduceLROnPlateau(factor=0.5, patience=5)
]

history = model.fit(
    train_dataset,
    validation_data=val_dataset,
    epochs=50,
    callbacks=callbacks
)
```

### Paso 6: Evaluar

```python
from sklearn.metrics import r2_score

y_pred = model.predict(X_val)
y_pred_original = scaler.inverse_transform(y_pred)
y_val_original = scaler.inverse_transform(y_val)

r2 = r2_score(y_val_original, y_pred_original)
print(f"RÂ² Score: {r2:.4f}")

# Visualizar
plt.scatter(y_val_original, y_pred_original, alpha=0.5)
plt.plot([y_val_original.min(), y_val_original.max()],
         [y_val_original.min(), y_val_original.max()], 'k--')
plt.xlabel('Real')
plt.ylabel('Predicted')
plt.title(f'RÂ² = {r2:.4f}')
plt.show()
```

## ğŸ› Troubleshooting

**Problema:** Overfitting (train loss << val loss)
- **SoluciÃ³n:** Aumentar dropout, aÃ±adir L2 regularization, reducir capacidad del modelo

**Problema:** Underfitting (train loss y val loss altos)
- **SoluciÃ³n:** Unfreeze mÃ¡s capas, aumentar lr, aÃ±adir capas en head

**Problema:** Nan loss
- **SoluciÃ³n:** Reducir lr, verificar normalizaciÃ³n de datos, clip gradients

**Problema:** GPU Out of Memory
- **SoluciÃ³n:** Reducir batch_size, usar model de menor tamaÃ±o (B0 en vez de B7)

## ğŸ“ˆ Mejores PrÃ¡cticas

1. **âœ… Siempre usar validation set** para early stopping
2. **âœ… Guardar solo best model** (save_best_only=True)
3. **âœ… Normalizar targets** con MinMaxScaler o StandardScaler
4. **âœ… Experimentar con diferentes freezing strategies**
5. **âœ… Monitorear mÃ©tricas fÃ­sicamente interpretables** (no solo loss)
6. **âœ… Visualizar predicciones** durante entrenamiento

## ğŸ”— Siguiente Paso

Una vez entrenado el modelo:
â†’ Ir a `../Results/` para anÃ¡lisis de interpretabilidad

---

**Nota:** Los notebooks incluyen cÃ³digo completo de entrenamiento. Esta documentaciÃ³n resume la metodologÃ­a comÃºn a todos.
